[{"title":"LLama2大模型量化部署","url":"/forward/1f77414f.html","content":"<h1 id=\"Llama模型量化模型cpp部署\"><a href=\"#Llama模型量化模型cpp部署\" class=\"headerlink\" title=\"Llama模型量化模型cpp部署\"></a>Llama模型量化模型cpp部署</h1><h2 id=\"部署步骤\"><a href=\"#部署步骤\" class=\"headerlink\" title=\"部署步骤\"></a>部署步骤</h2><p>wsl下部署没啥装个Ubuntu22.04先。</p>\n<p>然后git clone两个项目。</p>\n<p>首先是llama的git项目</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/facebookresearch/llama.git</span><br></pre></td></tr></table></figure>\n\n<p>然后是cpp</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br></pre></td></tr></table></figure>\n\n<p>然后进入llama的项目</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> llama</span><br></pre></td></tr></table></figure>\n\n<p>然后去官网<a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p>\n<p>申请下载到邮箱</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./download.sh</span><br></pre></td></tr></table></figure>\n\n<p>然后就是这样子</p>\n<p><img src=\"/images/pasted-1.png\" alt=\"upload successful\"></p>\n<p>等待模型下载完毕以后，进入llama.cpp</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> llama.cpp</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>先安装gcc环境</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo apt install build-essential</span><br></pre></td></tr></table></figure>\n\n<p>编译等待编译完成</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n\n<p>安装python依赖</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">python3 -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure>\n\n<p>转换模型，当然还有其他参数我们可以直接打开convert.py去查看</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">python3 convert.py <span class=\"string\">&#x27;模型地址&#x27;</span></span><br><span class=\"line\">python3 convert.py --outfile <span class=\"string\">&#x27;输出地址&#x27;</span> <span class=\"string\">&#x27;模型地址&#x27;</span></span><br><span class=\"line\">python3 convert.py --outfile ./models/llama-2-7b-chat ../llama/llama-2-7b-chat/</span><br></pre></td></tr></table></figure>\n\n<p>那么如果遇到以下问题</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">```bash</span><br><span class=\"line\">python3 haConvert.py --outfile ./models/llama-2-7b-chat ../llama/llama-2-7b-chat/</span><br><span class=\"line\">```</span><br><span class=\"line\">```error</span><br><span class=\"line\">Writing models/llama-2-7b-chat, format 1</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 1210, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    main()</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 1205, <span class=\"keyword\">in</span> main</span><br><span class=\"line\">    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 909, <span class=\"keyword\">in</span> write_all</span><br><span class=\"line\">    check_vocab_size(params, vocab)</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 796, <span class=\"keyword\">in</span> check_vocab_size</span><br><span class=\"line\">    raise Exception(msg)</span><br><span class=\"line\">Exception: Vocab size mismatch (model has -1, but ../llama/tokenizer.model has 32000).</span><br><span class=\"line\">```</span><br></pre></td></tr></table></figure>\n\n<p>不要去相信网上的added_tokens.json</p>\n<p>实际上在llama-2-7b-chat文件夹中，应该有一个.json文件（可能是params.json）。打开这个json文件，将”vocab_size”从-1改为32000。</p>\n<p><img src=\"/images/pasted-0.png\" alt=\"upload successful\"></p>\n<h2 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h2><p>实际上最后输出的是一个.bin文件，所以我们上面的命令是有瑕疵的。</p>\n<p>所以当我们转换成功以后。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> models</span><br><span class=\"line\"><span class=\"built_in\">mkdir</span> 7B</span><br><span class=\"line\"><span class=\"built_in\">mv</span> llama-2-7b-chat 7B/ggml-model-f16.bin</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"量化\"><a href=\"#量化\" class=\"headerlink\" title=\"量化\"></a>量化</h2><p>我们上面改为了bin这里也变成bin，我们执行的是4bit量化, 输出到./models/7B/目录下面</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.gguf q4_0</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h2><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./main -m ./models/7B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r <span class=\"string\">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure>","categories":["探索"],"tags":["大模型应用"]},{"title":"Langchain系列[01]介绍","url":"/forward/efdbd4e2.html","content":"<p>大家好，这里是<strong>粥余</strong>。<br>随着大模型技术的飞速发展，**<code>langchain</code>** 的迭代也来到了<strong>2.0</strong> 时代。<br>按照Langchain 新的文档结构再结合之前的资料，我们重新来整理一下相关知识。</p>\n<hr>\n<p>先来看下官网介绍： <a href=\"https://python.langchain.com/v0.2/docs/introduction/\">传送门</a></p>\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p><strong>LangChain</strong> 是一个开发由大型语言模型（LLMs）驱动的应用程序的框架。</p>\n<p>LangChain 简化了 LLM 应用生命周期的每一个阶段：</p>\n<ul>\n<li><strong>开发</strong>：使用 LangChain 的开源 <a href=\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\">构建块</a> 和 <a href=\"https://python.langchain.com/v0.2/docs/concepts/\">组件</a> 构建您的应用程序。利用 <a href=\"https://python.langchain.com/v0.2/docs/integrations/platforms/\">第三方集成</a> 和 <a href=\"https://python.langchain.com/v0.2/docs/templates/\">模板</a> 快速上手。</li>\n<li><strong>生产化</strong>：使用 <a href=\"https://docs.smith.langchain.com/\">LangSmith</a> 来检查、监控和评估您的链，以便您可以持续优化并自信地部署。</li>\n<li><strong>部署</strong>：使用 <a href=\"https://python.langchain.com/v0.2/docs/langserve/\">LangServe</a> 将任何链转变为 API。</li>\n</ul>\n<p><img src=\"/images/Langchain%E7%B3%BB%E5%88%97-01-%E4%BB%8B%E7%BB%8D.assets/image-20240627095511759.png\" alt=\"image-20240627095511759\"></p>\n<p>具体来说，该框架包括以下开源库：</p>\n<ul>\n<li>**<code>langchain-core</code>**：基本抽象和 LangChain 表达式语言。</li>\n<li>**<code>langchain-community</code>**：第三方集成。</li>\n<li>**<code>langchain</code>**：构成应用程序认知架构的链、代理和检索策略。</li>\n<li>**<a href=\"https://langchain-ai.github.io/langgraph\">langgraph</a>**：通过将步骤建模为图中的边和节点，使用 LLMs 构建健壮且具有状态的多参与者应用程序。</li>\n<li>**<a href=\"https://python.langchain.com/v0.2/docs/langserve/\">langserve</a>**：将 LangChain 链作为 REST API 部署。</li>\n</ul>\n<hr>\n<p>咱们先简单的概括一下：</p>\n<p><a href=\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\">构建块</a></p>\n<p>就是LangChain Expression Language (LCEL)， 也就是链中的各个组件:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Type</th>\n<th>Output Type</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prompt</td>\n<td>Dictionary</td>\n<td>PromptValue</td>\n<td>提示词</td>\n</tr>\n<tr>\n<td>ChatModel</td>\n<td>Single string, list of chat messages or a PromptValue</td>\n<td>ChatMessage</td>\n<td>chat 模型（支持多轮）</td>\n</tr>\n<tr>\n<td>LLM</td>\n<td>Single string, list of chat messages or a PromptValue</td>\n<td>String</td>\n<td>completion （单轮）</td>\n</tr>\n<tr>\n<td>OutputParser</td>\n<td>The output of an LLM or ChatModel</td>\n<td>Depends on the parser</td>\n<td>输出解析器</td>\n</tr>\n<tr>\n<td>Retriever</td>\n<td>Single string</td>\n<td>List of Documents</td>\n<td>检索器</td>\n</tr>\n<tr>\n<td>Tool</td>\n<td>Single string or dictionary, depending on the tool</td>\n<td>Depends on the tool</td>\n<td>工具</td>\n</tr>\n</tbody></table>\n<p><strong>提示词</strong>： 很好理解，给模型的指令，完成某个任务，比如让模型写一首诗。</p>\n<p><strong>ChatModel</strong> ：使用消息序列作为输入并返回聊天消息作为输出（而不是使用纯文本）的语言模型。聊天模型支持为对话消息分配不同的角色，帮助区分来自AI、用户以及系统消息等指令的消息。用户和AI对话，支持聊天历史，AI有记忆。</p>\n<p><strong>LLM</strong>:以字符串作为输入并返回字符串的语言模型。AI没有记忆，通常是一锤子买卖，用于完成一次性任务。比如翻译一句话。</p>\n<p>注意：</p>\n<p><strong><code>ChatModel</code></strong> 类似于 <strong>OpenAI Chat</strong></p>\n<p><strong><code>LLM</code></strong> 类似于 <strong>OpenAI Completions</strong></p>\n<p><strong>Chat vs Completions</strong></p>\n<p>OpenAI的ChatCompletion和Completion都是自然语言生成模型的接口，但它们的用途和应用场景略有不同。</p>\n<h4 id=\"Completions\"><a href=\"#Completions\" class=\"headerlink\" title=\"Completions\"></a>Completions</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">通用的自然语言生成接口，支持生成各种类型的文本，包括段落、摘要、建议、答案等等。</span><br><span class=\"line\"></span><br><span class=\"line\">Completion接口的输出更为多样化，可能会更加严谨和专业，适用于各种文本生成场景，例如文章创作、信息提取、机器翻译、自然语言问题回答等等。</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Chat\"><a href=\"#Chat\" class=\"headerlink\" title=\"Chat\"></a>Chat</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">专为生成对话和聊天场景而设计。</span><br><span class=\"line\"></span><br><span class=\"line\">ChatCompletion接口生成的文本通常会更具有人类对话的风格和语调，可以用于智能客服、聊天机器人等场景，以及在日常聊天中帮助用户自动生成回复。</span><br></pre></td></tr></table></figure>\n\n<p><strong>输出解析器</strong>：处理大模型的输出，可以将输出转化成需要的格式，比如转化成特定的JSON格式，便于后续流程处理。</p>\n<p><strong>检索器</strong>：在langchain中，检索器通常和<strong>向量数据库</strong>、<strong>嵌入模型</strong>绑定，用于语义搜索。比如可以在本地向量数据库Chroma 中使用BGE-large的中文模型，对中文数据进行相似度搜索。</p>\n<p>工具： 通常是我们自己定义的，能够实现某种功能的方法。工具可以被大模型识别，当用户的需求需要 调用工具时，大模型会自动识别，并让我们调用工具。Langchain 中的Agent 可以自己识别并调用用户定义的工具。</p>\n<hr>\n<p><a href=\"https://python.langchain.com/v0.2/docs/concepts/\">组件</a>：</p>\n<p>这里的组件，是指langchain 工具包含的各个重要的“包”。主要是下面这个几个包：</p>\n<p><strong>langchain-core</strong></p>\n<p>这个包包含了不同组件的基础抽象以及如何将它们组合在一起的方法。在这里定义了LLMs、vectorstores、retrievers等核心组件的接口。</p>\n<p><strong>langchain</strong></p>\n<p>主要的langchain包，包含了构成应用程序认知架构的链、代理和检索策略。这里的所有链、代理和检索策略都不是特定于任何一种集成，而是适用于所有集成的通用类型。</p>\n<p><strong>langchain-community</strong></p>\n<p>这个包包含了由LangChain社区维护的第三方集成。关键的合作伙伴包已经被分离出去。这个包包含了各种组件（LLMs、vectorstores、retrievers）的所有集成。为了尽可能保持包的轻量级，这个包中的所有依赖关系都是可选的。各个模型厂商、向量数据库厂商的包，都在这里。</p>\n<p><strong>LangGraph</strong></p>\n<p>langgraph是langchain的一个扩展，旨在通过将步骤建模为图中的边和节点，使用LLMs构建健壮且具有状态的多参与者应用程序。</p>\n<p>提供了创建常见类型代理的<strong>高级接口</strong>，以及用于组合自定义流程的API。</p>\n<p><strong>也是在langchain中构建复杂Agent 逻辑的基础。</strong>（Langchain 的部分现成的Agent 就是用LangGraph 实现）</p>\n<p><strong>langserve</strong></p>\n<p>一个用于将LangChain链作为REST APIs部署的包。它使得快速启动并运行一个生产就绪的API变得简单。能够让我们快速搭建AI应用的前后端。</p>\n<p><strong>LangSmith</strong></p>\n<p>一个开发者平台，可以调试、测试、评估和监控LLM应用程序。</p>\n<hr>\n<p><a href=\"https://python.langchain.com/v0.2/docs/integrations/platforms/\">第三方集成</a> ：</p>\n<p>只要不是langchain 自己开发的，包括 大语言模型、嵌入模型、向量数据库、第三方工具等等，都属于第三方集成。</p>\n<p><a href=\"https://python.langchain.com/v0.2/docs/templates/\">模板</a>：</p>\n<p>langchain 提供的一些可以开箱体验的功能，类似于脚手架，安装之后可以立即体验。</p>\n<hr>\n<p>在正式接触langchain 之前，小伙伴们可以先到国内大厂的网站注册一下，Langchain官方的代码里</p>\n<p>国内模型作为例子的情况不多，我们主要使用国内的模型给大家介绍，小伙伴们需要先去注册一下。</p>\n<p>国内比较不错的模型（性价比高，部分模型免费）：</p>\n<p><strong>百度千帆大模型平台</strong> ： <a href=\"https://qianfan.cloud.baidu.com/\">传送门</a></p>\n<ul>\n<li>ERNIE 4.0 文心一言4</li>\n<li>ERNIE 3.5 文心一言3.5</li>\n<li>ERNIE Speed <strong>免费</strong></li>\n<li>ERNIE Lite <strong>免费</strong></li>\n<li>ERNIE Tiny <strong>免费</strong></li>\n</ul>\n<p><strong>智谱AI</strong> ： <a href=\"https://open.bigmodel.cn/\">传送门</a></p>\n<ul>\n<li>GLM-4-0520</li>\n<li>GLM-4-AirX</li>\n<li>GLM-4-Air</li>\n</ul>\n<p>**通义千问 （**DashScope灵积模型服务**）**： <a href=\"https://help.aliyun.com/zh/dashscope/developer-reference/api-details\">传送门</a></p>\n<ul>\n<li>qwen-turbo</li>\n<li>qwen-plus</li>\n<li>qwen-max</li>\n</ul>\n<p><strong>字节跳动 豆包模型</strong>： <a href=\"https://www.volcengine.com/product/doubao\">传送门</a></p>\n<ul>\n<li>Doubao-lite-4k</li>\n<li>Doubao-pro-32k</li>\n</ul>\n<p><strong>Moonshot</strong>：<a href=\"https://platform.moonshot.cn/docs/intro#%E4%B8%BB%E8%A6%81%E6%A6%82%E5%BF%B5\"> 传送门</a></p>\n<ul>\n<li>moonshot-v1-128k</li>\n<li>moonshot-v1-32k</li>\n</ul>\n","tags":["langchain"]},{"title":"Langchain系列[02]使用LCEL构建一个简单的LLM应用程序","url":"/forward/24d30ec1.html","content":"<h1 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h1><p>今天咱们介绍一下如何使用 LangChain 构建一个简单的 LLM 应用程序。</p>\n<p>这个应用程序能够将英文文本翻译成其他语言。</p>\n<p>这是一个相对简单的 LLM 应用程序 —— 它仅包含一个 LLM 调用和一些提示。</p>\n<p>今天的学习目的是初步了解下面的概念</p>\n<ul>\n<li>使用语言模型 （必须）</li>\n<li>使用 PromptTemplates 和 OutputParsers（必须）</li>\n<li>使用 LangChain 表达式语言 (LCEL) 将组件连接在一起（必须）</li>\n<li>使用 LangSmith 调试和跟踪您的应用程序 （不用也问题不大）</li>\n<li>使用 LangServe 部署您的应用程序（不用也问题不大）</li>\n</ul>\n<h2 id=\"准备阶段\"><a href=\"#准备阶段\" class=\"headerlink\" title=\"准备阶段\"></a>准备阶段</h2><p>我们全程使用windows + python, 其他的方法就不介绍了，同学们可以自己探索~</p>\n<p><strong>安装</strong></p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\">pip install langchain</span><br></pre></td></tr></table></figure>\n\n<p><strong>关于 Jupyter Notebook</strong></p>\n<p>咱们用能跑python的编程工具就可以，用什么来跑不是很重要-。-</p>\n<p><strong><a href=\"https://www.langchain.com/langsmith\">LangSmith</a></strong></p>\n<p>langsmith 是一个可以跟踪runnable  的工具，可以跟踪和记录复杂流程中的大部分参数，对于复杂问题分析很有帮助。在学习初期，我们可以先不使用这个工具，到后面遇到复杂逻辑和chain的时候，我们再使用。 感兴趣的同学可以自己去注册一下，每月有免费的使用额度。</p>\n<p><strong>薅羊毛</strong> langsmith 的提示：截止2024 7.1日前，免费账户每月有5K 的使用次数</p>\n<p>New Pricing:All free accounts will be rate limited to 5k traces per month starting on July 1st. To avoid this, <a href=\"https://smith.langchain.com/settings/payments\">sign up</a> for our paid plans</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\">你可以这样简单使用</span><br><span class=\"line\"># 新建一个<span class=\"variable constant_\">ID</span>号，用于记录你的<span class=\"title class_\">Log</span></span><br><span class=\"line\">unique_id = <span class=\"title function_\">uuid4</span>().<span class=\"property\">hex</span>[<span class=\"number\">0</span>:<span class=\"number\">8</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"># 写一个<span class=\"variable constant_\">TAG</span></span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;LANGCHAIN_PROJECT&quot;</span>] = f<span class=\"string\">&quot; Agent RAG - &#123;unique_id&#125;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"># 这个表示你要使用langsmith 开始记录，不用就<span class=\"literal\">false</span></span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;LANGCHAIN_TRACING_V2&quot;</span>] = <span class=\"string\">&#x27;true&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"># 这个是你注册langsmith 后的<span class=\"variable constant_\">KEY</span></span><br><span class=\"line\"># 你可以把你的key 放到环境中，用<span class=\"variable constant_\">MY_LANGCHAIN_API_KEY</span>来引用</span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;LANGCHAIN_API_KEY&quot;</span>] = os.<span class=\"title function_\">getenv</span>(<span class=\"string\">&#x27;MY_LANGCHAIN_API_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"># 要是你觉得麻烦 你就这样</span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;LANGCHAIN_API_KEY&quot;</span>] = <span class=\"string\">&#x27;dsad6487dfdhb...  你的key&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"使用语言模型\"><a href=\"#使用语言模型\" class=\"headerlink\" title=\"使用语言模型\"></a>使用语言模型</h2><p>官网列出一堆模型，好是好，国内能用的没几个…(不翻墙的情况)</p>\n<p>我们先用官网中的 <strong>Azure  OpenAi</strong>  以及 <strong>千帆</strong>来做演示~</p>\n<h3 id=\"创建一个-Azure-OpenAi-chatModel\"><a href=\"#创建一个-Azure-OpenAi-chatModel\" class=\"headerlink\" title=\"创建一个  Azure OpenAi chatModel\"></a><strong>创建一个  Azure OpenAi chatModel</strong></h3><figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;AZURE_OPENAI_API_KEY&quot;</span>] =<span class=\"variable constant_\">AZURE_OPENAI_API_KEY</span></span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;AZURE_OPENAI_ENDPOINT&quot;</span>] =<span class=\"variable constant_\">AZURE_OPENAI_ENDPOINT</span></span><br><span class=\"line\">gpt3p5_model = <span class=\"title class_\">AzureChatOpenAI</span>(</span><br><span class=\"line\">    openai_api_version=<span class=\"string\">&quot;2024-02-15-preview&quot;</span>,</span><br><span class=\"line\">    azure_deployment=<span class=\"variable constant_\">DEPLOYMENT_NAME_GPT3P5</span>,</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<p><strong>AZURE_OPENAI_API_KEY</strong>: 你的<strong>Azure OpenAI KEY,</strong> 需要在微软Azure中申请订阅</p>\n<p><strong>AZURE_OPENAI_ENDPOINT</strong>： 你的服务端endpit,一个url地址，订阅后从Azure获取</p>\n<p><strong>openai_api_version</strong>： 你使用模型的版本号，依赖你使用的模型</p>\n<p><strong>azure_deployment</strong>：你给你部署的模型起的名字，从Azure获取</p>\n<p><strong>给出提示词，并调用invoke (LCEL 的方法之一)</strong></p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\">messages = [</span><br><span class=\"line\">    <span class=\"title class_\">SystemMessage</span>(content=<span class=\"string\">&quot;Translate the following from English into Italian&quot;</span>),</span><br><span class=\"line\">    <span class=\"title class_\">HumanMessage</span>(content=<span class=\"string\">&quot;hi!&quot;</span>),</span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">res = gpt3p5_model.<span class=\"title function_\">invoke</span>(messages)</span><br><span class=\"line\"><span class=\"title function_\">print</span>(res.<span class=\"property\">content</span>)</span><br></pre></td></tr></table></figure>\n\n<p>messages： 消息列表，通常包含整个对话历史</p>\n<p>SystemMessage：系统提示词，在messages 中一般只出现一次，表明系统的<strong>身份</strong></p>\n<p>HumanMessage： 用户消息</p>\n<p><strong>输出</strong></p>\n<p>模型输出的内容是 一个 <strong>AIMessage</strong></p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">AIMessage</span>(content=<span class=\"string\">&#x27;ciao!....</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">content=&#x27;</span><span class=\"title class_\">Ciao</span>!<span class=\"string\">&#x27; response_metadata=&#123;&#x27;</span>token_usage<span class=\"string\">&#x27;: &#123;&#x27;</span>completion_tokens<span class=\"string\">&#x27;: 3, &#x27;</span>prompt_tokens<span class=\"string\">&#x27;: 20, &#x27;</span>total_tokens<span class=\"string\">&#x27;: 23&#125;, &#x27;</span>model_name<span class=\"string\">&#x27;: &#x27;</span>gpt-<span class=\"number\">35</span>-turbo<span class=\"string\">&#x27;, &#x27;</span>system_fingerprint<span class=\"string\">&#x27;: None, &#x27;</span>prompt_filter_results<span class=\"string\">&#x27;: [&#123;&#x27;</span>prompt_index<span class=\"string\">&#x27;: 0, &#x27;</span>content_filter_results<span class=\"string\">&#x27;: &#123;&#x27;</span>hate<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;, &#x27;</span>self_harm<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;, &#x27;</span>sexual<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;, &#x27;</span>violence<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;&#125;&#125;], &#x27;</span>finish_reason<span class=\"string\">&#x27;: &#x27;</span>stop<span class=\"string\">&#x27;, &#x27;</span>logprobs<span class=\"string\">&#x27;: None, &#x27;</span>content_filter_results<span class=\"string\">&#x27;: &#123;&#x27;</span>hate<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;, &#x27;</span>self_harm<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;, &#x27;</span>sexual<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;, &#x27;</span>violence<span class=\"string\">&#x27;: &#123;&#x27;</span>filtered<span class=\"string\">&#x27;: False, &#x27;</span>severity<span class=\"string\">&#x27;: &#x27;</span>safe<span class=\"string\">&#x27;&#125;&#125;&#125; id=&#x27;</span>run-2340cccd-9b63-4b07-9dfd-3c154bbef121-<span class=\"number\">0</span><span class=\"string\">&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建-文心一言4-chatModel\"><a href=\"#创建-文心一言4-chatModel\" class=\"headerlink\" title=\"创建 文心一言4 chatModel\"></a>创建 文心一言4 chatModel</h3><figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\"># 千帆的 <span class=\"variable constant_\">AK</span> 和 <span class=\"variable constant_\">SK</span>， 需要我们去百度网站申请</span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;QIANFAN_AK&quot;</span>] = <span class=\"variable constant_\">MY_QIANFAN_AK</span></span><br><span class=\"line\">os.<span class=\"property\">environ</span>[<span class=\"string\">&quot;QIANFAN_SK&quot;</span>] = <span class=\"variable constant_\">MY_QIANFAN_SK</span></span><br><span class=\"line\"></span><br><span class=\"line\"># 文心一言<span class=\"number\">4</span> 模型<span class=\"variable constant_\">ERNIE</span>-<span class=\"title class_\">Bot</span>-<span class=\"number\">4</span></span><br><span class=\"line\">wenxin4_model = <span class=\"title class_\">QianfanChatEndpoint</span>(model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span>)</span><br><span class=\"line\">res = wenxin4_model.<span class=\"title function_\">invoke</span>(messages)</span><br><span class=\"line\"><span class=\"title function_\">print</span>(res.<span class=\"property\">content</span>)</span><br><span class=\"line\">pass</span><br></pre></td></tr></table></figure>\n\n<p>输出</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"title class_\">AIMessage</span>(content=<span class=\"string\">&#x27;salve!....</span></span><br><span class=\"line\"><span class=\"string\">content=&#x27;</span>salve!<span class=\"string\">&#x27; additional_kwargs=&#123;&#x27;</span>finish_reason<span class=\"string\">&#x27;: &#x27;</span>normal<span class=\"string\">&#x27;, &#x27;</span>request_id<span class=\"string\">&#x27;: &#x27;</span><span class=\"keyword\">as</span>-hpbfyf1uqr<span class=\"string\">&#x27;, &#x27;</span>object<span class=\"string\">&#x27;: &#x27;</span>chat.<span class=\"property\">completion</span><span class=\"string\">&#x27;, &#x27;</span>search_info<span class=\"string\">&#x27;: [], &#x27;</span>function_call<span class=\"string\">&#x27;: &#123;&#125;, &#x27;</span>tool_calls<span class=\"string\">&#x27;: [&#123;&#x27;</span>type<span class=\"string\">&#x27;: &#x27;</span><span class=\"keyword\">function</span><span class=\"string\">&#x27;, &#x27;</span><span class=\"keyword\">function</span><span class=\"string\">&#x27;: &#123;&#125;&#125;]&#125; response_metadata=&#123;&#x27;</span>token_usage<span class=\"string\">&#x27;: &#123;&#x27;</span>prompt_tokens<span class=\"string\">&#x27;: 10, &#x27;</span>completion_tokens<span class=\"string\">&#x27;: 3, &#x27;</span>total_tokens<span class=\"string\">&#x27;: 13&#125;, &#x27;</span>model_name<span class=\"string\">&#x27;: &#x27;</span><span class=\"variable constant_\">ERNIE</span>-<span class=\"title class_\">Bot</span>-<span class=\"number\">4</span><span class=\"string\">&#x27;, &#x27;</span>finish_reason<span class=\"string\">&#x27;: &#x27;</span>normal<span class=\"string\">&#x27;, &#x27;</span>id<span class=\"string\">&#x27;: &#x27;</span><span class=\"keyword\">as</span>-hpbfyf1uqr<span class=\"string\">&#x27;, &#x27;</span>object<span class=\"string\">&#x27;: &#x27;</span>chat.<span class=\"property\">completion</span><span class=\"string\">&#x27;, &#x27;</span>created<span class=\"string\">&#x27;: 1718674042, &#x27;</span>result<span class=\"string\">&#x27;: &#x27;</span>salve!<span class=\"string\">&#x27;, &#x27;</span>is_truncated<span class=\"string\">&#x27;: False, &#x27;</span>need_clear_history<span class=\"string\">&#x27;: False, &#x27;</span>usage<span class=\"string\">&#x27;: &#123;&#x27;</span>prompt_tokens<span class=\"string\">&#x27;: 10, &#x27;</span>completion_tokens<span class=\"string\">&#x27;: 3, &#x27;</span>total_tokens<span class=\"string\">&#x27;: 13&#125;&#125; id=&#x27;</span>run-4f50bc84-86a6-46dd-8c6f-03d0fbafc3d5-<span class=\"number\">0</span><span class=\"string\">&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"输出解析器\"><a href=\"#输出解析器\" class=\"headerlink\" title=\"输出解析器\"></a>输出解析器</h3><figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.<span class=\"property\">output_parsers</span> <span class=\"keyword\">import</span> <span class=\"title class_\">StrOutputParser</span></span><br><span class=\"line\"></span><br><span class=\"line\">parser = <span class=\"title class_\">StrOutputParser</span>()</span><br><span class=\"line\"></span><br><span class=\"line\"># 大模型的输出</span><br><span class=\"line\">result = model.<span class=\"title function_\">invoke</span>(messages)</span><br><span class=\"line\"></span><br><span class=\"line\"># 解析器 解析模型的输出</span><br><span class=\"line\">parser.<span class=\"title function_\">invoke</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\"># 将 <span class=\"title class_\">AIMessage</span> 解析成 字符串  （这一步要理解一下...）</span><br><span class=\"line\"><span class=\"string\">&#x27;Ciao!&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<p>为了让上面几步连贯，这也是langchain 的 精华之一</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"code\"><pre><span class=\"line\"># 使用 chain</span><br><span class=\"line\">chain = model | parser</span><br></pre></td></tr></table></figure>\n\n<p>于是 文心一言4 的chain， 可以这样</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">chain = wenxin4_model | parser</span><br><span class=\"line\">res = chain.invoke(messages)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res.content)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"使用提示词模板\"><a href=\"#使用提示词模板\" class=\"headerlink\" title=\"使用提示词模板\"></a>使用提示词模板</h3><p>我们可以在提示词中使用<strong>变量，</strong>变量的表示方法是把变量放到{ }中</p>\n<p>系统提示词： 通过变量language， 我们可以改变目标语言的类型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">system_template = <span class=\"string\">&quot;Translate the following into &#123;language&#125;:&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>将用户输入，放入变量 text;组合成一个[]</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">prompt_template = ChatPromptTemplate.from_messages(</span><br><span class=\"line\">    [(<span class=\"string\">&quot;system&quot;</span>, system_template), (<span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;&#123;text&#125;&quot;</span>)]</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<p>通过invoke 方法，指定变量的内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">result = prompt_template.invoke(&#123;<span class=\"string\">&quot;language&quot;</span>: <span class=\"string\">&quot;italian&quot;</span>, <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;hi&quot;</span>&#125;)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result.to_messages())</span><br></pre></td></tr></table></figure>\n\n<p><strong>输出</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">messages=[SystemMessage(content=<span class=\"string\">&#x27;Translate the following into italian:&#x27;</span>), HumanMessage(content=<span class=\"string\">&#x27;hi&#x27;</span>)]</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"将上面的各个部分组合起来：-chain\"><a href=\"#将上面的各个部分组合起来：-chain\" class=\"headerlink\" title=\"将上面的各个部分组合起来： chain !\"></a>将上面的各个部分组合起来： chain !</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">ful_chain = prompt_template | wenxin4_model | parser</span><br><span class=\"line\">res = ful_chain.invoke(&#123;<span class=\"string\">&quot;language&quot;</span>: <span class=\"string\">&quot;italian&quot;</span>, <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;hi&quot;</span>&#125;)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"初步体验一下-Langserve\"><a href=\"#初步体验一下-Langserve\" class=\"headerlink\" title=\"初步体验一下 Langserve\"></a>初步体验一下 Langserve</h2><p>核心</p>\n<p>用 FastAPI 创建一个APP，再调用 langserve add_routes方法，将消息路由到chain 上，作为chain 的输入</p>\n<p>需要安装</p>\n<p>pip install sse_starlette</p>\n<p>pip install langserve</p>\n<p>文心4 作为模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/env python</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">from</span> typing <span class=\"keyword\">import</span> <span class=\"type\">List</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> fastapi <span class=\"keyword\">import</span> FastAPI</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_openai <span class=\"keyword\">import</span> ChatOpenAI</span><br><span class=\"line\"><span class=\"keyword\">from</span> langserve <span class=\"keyword\">import</span> add_routes</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> llm_cfg <span class=\"keyword\">import</span> MY_QIANFAN_AK, MY_QIANFAN_SK</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. Create prompt template</span></span><br><span class=\"line\">system_template = <span class=\"string\">&quot;Translate the following into &#123;language&#125;:&quot;</span></span><br><span class=\"line\">prompt_template = ChatPromptTemplate.from_messages([</span><br><span class=\"line\">    (<span class=\"string\">&#x27;system&#x27;</span>, system_template),</span><br><span class=\"line\">    (<span class=\"string\">&#x27;user&#x27;</span>, <span class=\"string\">&#x27;&#123;text&#125;&#x27;</span>)</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. Create model</span></span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;QIANFAN_AK&quot;</span>] = MY_QIANFAN_AK</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;QIANFAN_SK&quot;</span>] = MY_QIANFAN_SK</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 千帆</span></span><br><span class=\"line\">model = QianfanChatEndpoint(model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. Create parser</span></span><br><span class=\"line\">parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. Create chain</span></span><br><span class=\"line\">chain = prompt_template | model | parser</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. App definition</span></span><br><span class=\"line\">app = FastAPI(</span><br><span class=\"line\">  title=<span class=\"string\">&quot;LangChain Server&quot;</span>,</span><br><span class=\"line\">  version=<span class=\"string\">&quot;1.0&quot;</span>,</span><br><span class=\"line\">  description=<span class=\"string\">&quot;A simple API server using LangChain&#x27;s Runnable interfaces&quot;</span>,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. Adding chain route</span></span><br><span class=\"line\"></span><br><span class=\"line\">add_routes(</span><br><span class=\"line\">    app,</span><br><span class=\"line\">    chain,</span><br><span class=\"line\">    path=<span class=\"string\">&quot;/chain&quot;</span>,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    <span class=\"keyword\">import</span> uvicorn</span><br><span class=\"line\"></span><br><span class=\"line\">    uvicorn.run(app, host=<span class=\"string\">&quot;localhost&quot;</span>, port=<span class=\"number\">8000</span>)</span><br></pre></td></tr></table></figure>\n\n<p>直接运行main 方法，看到命令行成功后，浏览器中打开 ：<a href=\"http://localhost:8000/chain/playground/\">http://localhost:8000/chain/playground/</a></p>\n<h4 id=\"客户端调用服务端\"><a href=\"#客户端调用服务端\" class=\"headerlink\" title=\"客户端调用服务端\"></a>客户端调用服务端</h4><p>有了上面的服务端，Langchain 可以支持远程chain的调用，小伙伴们可以自行尝试。</p>\n<p>在单位里，小伙伴们可以自己建立一个大模型服务，然后让单位里的小伙伴调用~~</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> langserve <span class=\"keyword\">import</span> RemoteRunnable</span><br><span class=\"line\"></span><br><span class=\"line\">remote_chain = RemoteRunnable(<span class=\"string\">&quot;http://localhost:8000/chain/&quot;</span>)</span><br><span class=\"line\">remote_chain.invoke(&#123;<span class=\"string\">&quot;language&quot;</span>: <span class=\"string\">&quot;italian&quot;</span>, <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;hi&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"代码附录\"><a href=\"#代码附录\" class=\"headerlink\" title=\"代码附录\"></a>代码附录</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_openai <span class=\"keyword\">import</span> AzureChatOpenAI</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.messages <span class=\"keyword\">import</span> HumanMessage, SystemMessage</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> llm_cfg <span class=\"keyword\">import</span> AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, DEPLOYMENT_NAME_GPT3P5, MY_QIANFAN_AK, MY_QIANFAN_SK</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;AZURE_OPENAI_API_KEY&quot;</span>] =AZURE_OPENAI_API_KEY</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;AZURE_OPENAI_ENDPOINT&quot;</span>] =AZURE_OPENAI_ENDPOINT</span><br><span class=\"line\">    gpt3p5_model = AzureChatOpenAI(</span><br><span class=\"line\">        openai_api_version=<span class=\"string\">&quot;2024-02-15-preview&quot;</span>,</span><br><span class=\"line\">        azure_deployment=DEPLOYMENT_NAME_GPT3P5,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    messages = [</span><br><span class=\"line\">        SystemMessage(content=<span class=\"string\">&quot;Translate the following from English into Italian&quot;</span>),</span><br><span class=\"line\">        HumanMessage(content=<span class=\"string\">&quot;hi!&quot;</span>),</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br><span class=\"line\">    res = gpt3p5_model.invoke(messages)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res.content)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_AK&quot;</span>] = MY_QIANFAN_AK</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_SK&quot;</span>] = MY_QIANFAN_SK</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 千帆</span></span><br><span class=\"line\">    wenxin4_model = QianfanChatEndpoint(model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span>)</span><br><span class=\"line\">    res = wenxin4_model.invoke(messages)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res.content)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 输出解析</span></span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser</span><br><span class=\"line\"></span><br><span class=\"line\">    parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># chain</span></span><br><span class=\"line\">    chain = wenxin4_model | parser</span><br><span class=\"line\">    res = chain.invoke(messages)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res.content)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词</span></span><br><span class=\"line\">    system_template = <span class=\"string\">&quot;Translate the following into &#123;language&#125;:&quot;</span></span><br><span class=\"line\">    prompt_template = ChatPromptTemplate.from_messages(</span><br><span class=\"line\">        [(<span class=\"string\">&quot;system&quot;</span>, system_template), (<span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;&#123;text&#125;&quot;</span>)]</span><br><span class=\"line\">    )</span><br><span class=\"line\">    result = prompt_template.invoke(&#123;<span class=\"string\">&quot;language&quot;</span>: <span class=\"string\">&quot;italian&quot;</span>, <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;hi&quot;</span>&#125;)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(result.to_messages())</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    ful_chain = prompt_template | wenxin4_model | parser</span><br><span class=\"line\">    res = ful_chain.invoke(&#123;<span class=\"string\">&quot;language&quot;</span>: <span class=\"string\">&quot;italian&quot;</span>, <span class=\"string\">&quot;text&quot;</span>: <span class=\"string\">&quot;hi&quot;</span>&#125;)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>"},{"title":"Hello World","url":"/forward/4a17b156.html","content":"<h2 id=\"失語時代下的喃喃自語\"><a href=\"#失語時代下的喃喃自語\" class=\"headerlink\" title=\"失語時代下的喃喃自語\"></a>失語時代下的喃喃自語</h2><p>纪念2017年愚人节，reddit网站发起一项为期三天的社会实验（2017年4月1日-4月3日），号召所有注册用户在一块100万像素的画布上作画（1000*1000），用户有16种像素颜色选择，生成一次后需要等待20分钟到5分钟之后才可以进行下一次编辑。凭借大家的协作创造出得一幅伟大的作品并载入互联网史册。</p>\n<p>在混乱中建立秩序，文明也在一次次破坏重建中焕发了新的面貌，当资源有限的情况下，一个群体想要生存势必要蚕食别的群体，生存还是毁灭的问题在短短的72小时内在一块小小的帆布画版上不断上演。虽然这个活动在几年前就已经结束了，但现实中比Reddit这场社会实验残酷百倍的故事却从未停歇。<br><del>国家之间的冲突，族群之间的恶意。在更大纬度的战场，渗透于各个领域的对垒，甚至已经关乎到十几亿几十亿人们的幸福生活。</del></p>\n<p>活动地址参见：<a href=\"https://www.reddit.com/r/place/\">reddit/r/place</a><br>活动详情参见：<a href=\"https://en.wikipedia.org/wiki/Place_(Reddit)\">维基百科</a><br>画板元素详解：<a href=\"https://draemm.li/various/place-atlas/\">The /r/place Atlas</a><br>变化过程参见：<a href=\"https://www.bilibili.com/video/BV1WW41197qY\">bilibil</a></p>\n<p>活动结束时最终快照[高清]：<br><img src=\"/images/img--1.png\" alt=\"reddit-place-2017\"></p>\n","tags":["place"]},{"title":"iterm2常用快捷键","url":"/forward/10883bf2.html","content":"<h1 id=\"Iterm2-常用快捷键\"><a href=\"#Iterm2-常用快捷键\" class=\"headerlink\" title=\"Iterm2 常用快捷键\"></a>Iterm2 常用快捷键</h1><p><img src=\"/images/iterm2%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE.assets/image-20240429133734551.png\" alt=\"image-20240429133734551\"></p>\n<h2 id=\"标签控制\"><a href=\"#标签控制\" class=\"headerlink\" title=\"标签控制\"></a>标签控制</h2><figure class=\"highlight text\"><table><tr><td class=\"code\"><pre><span class=\"line\">新建标签：command + t</span><br><span class=\"line\"></span><br><span class=\"line\">关闭标签：command + w</span><br><span class=\"line\"></span><br><span class=\"line\">切换标签：command + 数字 command + 左右方向键</span><br><span class=\"line\"></span><br><span class=\"line\">切换全屏：command + enter</span><br><span class=\"line\"></span><br><span class=\"line\">查找：command + f</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"分屏控制\"><a href=\"#分屏控制\" class=\"headerlink\" title=\"分屏控制\"></a>分屏控制</h2><figure class=\"highlight text\"><table><tr><td class=\"code\"><pre><span class=\"line\">垂直分屏：command + d</span><br><span class=\"line\"></span><br><span class=\"line\">水平分屏：command + shift + d</span><br><span class=\"line\"></span><br><span class=\"line\">切换屏幕：command + option + 方向键 command + [ 或 command + ]</span><br><span class=\"line\"></span><br><span class=\"line\">查看历史命令：command + ;</span><br><span class=\"line\"></span><br><span class=\"line\">查看剪贴板历史：command + shift + h</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"光标操作（常用）\"><a href=\"#光标操作（常用）\" class=\"headerlink\" title=\"光标操作（常用）\"></a>光标操作（常用）</h2><figure class=\"highlight text\"><table><tr><td class=\"code\"><pre><span class=\"line\">清除当前行（实际上是光标前全部内容）：ctrl + u</span><br><span class=\"line\"></span><br><span class=\"line\">删除前一个字符|删除后一个字符：ctrl + h | ctrl + h</span><br><span class=\"line\"></span><br><span class=\"line\">按单词往前删除（推荐记忆）：ctrl + w</span><br><span class=\"line\"></span><br><span class=\"line\">删除光标后所有内容：ctrl + k</span><br><span class=\"line\"></span><br><span class=\"line\">到行首（尾）：ctrl + a/e</span><br><span class=\"line\"></span><br><span class=\"line\">光标前进后退：-&gt; ctrl + f | &lt;- ctrl + b</span><br><span class=\"line\"></span><br><span class=\"line\">上一条命令|下一条命令：ctrl + p/n</span><br><span class=\"line\"></span><br><span class=\"line\">可以搜索的历史命令：ctrl + r</span><br><span class=\"line\"></span><br><span class=\"line\">清屏：command + r | ctrl + l | 我自己一般手动clear</span><br></pre></td></tr></table></figure>\n\n\n\n<p>上面就是我整理的一些快捷键。</p>\n<p>下面就是大整合：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">新建标签：<span class=\"built_in\">command</span> + t</span><br><span class=\"line\"></span><br><span class=\"line\">关闭标签：<span class=\"built_in\">command</span> + w</span><br><span class=\"line\"></span><br><span class=\"line\">切换标签：<span class=\"built_in\">command</span> + 数字 <span class=\"built_in\">command</span> + 左右方向键</span><br><span class=\"line\"></span><br><span class=\"line\">切换全屏：<span class=\"built_in\">command</span> + enter</span><br><span class=\"line\"></span><br><span class=\"line\">查找：<span class=\"built_in\">command</span> + f</span><br><span class=\"line\"></span><br><span class=\"line\">垂直分屏：<span class=\"built_in\">command</span> + d</span><br><span class=\"line\"></span><br><span class=\"line\">水平分屏：<span class=\"built_in\">command</span> + <span class=\"built_in\">shift</span> + d</span><br><span class=\"line\"></span><br><span class=\"line\">切换屏幕：<span class=\"built_in\">command</span> + option + 方向键 <span class=\"built_in\">command</span> + [ 或 <span class=\"built_in\">command</span> + ]</span><br><span class=\"line\"></span><br><span class=\"line\">查看历史命令：<span class=\"built_in\">command</span> + ;</span><br><span class=\"line\"></span><br><span class=\"line\">查看剪贴板历史：<span class=\"built_in\">command</span> + <span class=\"built_in\">shift</span> + h</span><br><span class=\"line\"></span><br><span class=\"line\">清除当前行：ctrl + u</span><br><span class=\"line\"></span><br><span class=\"line\">到行首：ctrl + a</span><br><span class=\"line\"></span><br><span class=\"line\">到行尾：ctrl + e</span><br><span class=\"line\"></span><br><span class=\"line\">前进后退：ctrl + f/b (相当于左右方向键)</span><br><span class=\"line\"></span><br><span class=\"line\">上一条命令：ctrl + p</span><br><span class=\"line\"></span><br><span class=\"line\">搜索命令历史：ctrl + r</span><br><span class=\"line\"></span><br><span class=\"line\">删除当前光标的字符：ctrl + d</span><br><span class=\"line\"></span><br><span class=\"line\">删除光标之前的字符：ctrl + h</span><br><span class=\"line\"></span><br><span class=\"line\">删除光标之前的单词：ctrl + w</span><br><span class=\"line\"></span><br><span class=\"line\">删除到文本末尾：ctrl + k</span><br><span class=\"line\"></span><br><span class=\"line\">交换光标处文本：ctrl + t</span><br><span class=\"line\"></span><br><span class=\"line\">清屏1：<span class=\"built_in\">command</span> + r</span><br><span class=\"line\"></span><br><span class=\"line\">清屏2：ctrl + l</span><br><span class=\"line\"></span><br><span class=\"line\">自带有哪些很实用的功能/快捷键</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + 数字在各 tab 标签直接来回切换</span><br><span class=\"line\"></span><br><span class=\"line\">选择即复制 + 鼠标中键粘贴，这个很实用</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + f 所查找的内容会被自动复制</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + d 横着分屏 / ⌘ + <span class=\"built_in\">shift</span> + d 竖着分屏</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏</span><br><span class=\"line\"></span><br><span class=\"line\">ctrl + u 清空当前行，无论光标在什么位置</span><br><span class=\"line\"></span><br><span class=\"line\">输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + <span class=\"built_in\">shift</span> + h 会列出剪切板历史</span><br><span class=\"line\"></span><br><span class=\"line\">可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n","categories":["折腾"],"tags":["MacOS终端"]},{"title":"操作系统杂项考点习题-软考版","url":"/forward/cff67e5b.html","content":"<h1 id=\"操作系统\"><a href=\"#操作系统\" class=\"headerlink\" title=\"操作系统\"></a>操作系统</h1><h2 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h2><p>通过资源管理，提高计算机系统效率。</p>\n<p>改善人机界面，向用户提供友好的工作环境。</p>\n<h2 id=\"特性\"><a href=\"#特性\" class=\"headerlink\" title=\"特性\"></a>特性</h2><ul>\n<li>并发性</li>\n<li>共享性</li>\n<li>异步性</li>\n</ul>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240429123939629.png\" alt=\"image-20240429123939629\"></p>\n<h2 id=\"进程管理\"><a href=\"#进程管理\" class=\"headerlink\" title=\"进程管理\"></a>进程管理</h2><h3 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h3><p>进程：</p>\n<ol>\n<li>进程是程序的一次执行</li>\n<li>进程是一个程序及其数据在处理机上顺序执行时发生的活动</li>\n<li>进程是具有独立功能的程序在一个数据集合上运行的过程，是系统进行资源分配和调度的独立单位</li>\n<li>也可以这么记，进程是进程实体的一次运行，是系统进行资源分配和调度的一个独立单位</li>\n</ol>\n<p>程序：</p>\n<p>​    程序是一组有序指令的合集并存在某种介质上，本身不具有活动含义。</p>\n<p>线程:</p>\n<p>​    线程是进程中的一个实体，是被系统独立调用的基本单位。 线程的资源用的是进程里面的。</p>\n<p>简单的流程图（我的灵魂手绘图）：</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504112732155.png\" alt=\"image-20240504112732155\"></p>\n<h3 id=\"进程控制\"><a href=\"#进程控制\" class=\"headerlink\" title=\"进程控制\"></a>进程控制</h3><p>原语：控制程序的指令段，要么不执行，要么都执行，不可分割的。</p>\n<p>同步：直接制约，是有顺序的。</p>\n<p>互斥：异步制约，双方都可制约。</p>\n<p>灵界资源：一次只能供一个进程使用的资源</p>\n<p>临界区：使用临界资源的代码片段。</p>\n<h3 id=\"信号量机制\"><a href=\"#信号量机制\" class=\"headerlink\" title=\"信号量机制\"></a>信号量机制</h3><p>信号量：</p>\n<ul>\n<li>一个整数</li>\n<li>S&gt;=0 表示资源的可用数</li>\n<li>S&lt;0 S的绝对值就是等待队列或是阻塞队列的任务书</li>\n</ul>\n<p>PV操作：</p>\n<p>P操作就是占用一个资源，S -= 1</p>\n<p>S操作就是任务结束了，释放一个资源S+=1</p>\n<h4 id=\"PV互斥模型\"><a href=\"#PV互斥模型\" class=\"headerlink\" title=\"PV互斥模型\"></a>PV互斥模型</h4><p>在一个程序段里，pv操作应该同时出现。</p>\n<p>P(S):</p>\n<p>使用XXX</p>\n<p>V(S):</p>\n<p>后续代码</p>\n<p>互斥信号量s的初始值为1</p>\n<h4 id=\"PV同步模型\"><a href=\"#PV同步模型\" class=\"headerlink\" title=\"PV同步模型\"></a>PV同步模型</h4><p>如单缓冲区的生产消费问题，其实也就是同一个信号量在不同的模型中进行控制</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">smophore mutex = <span class=\"number\">1</span> empty = n full = <span class=\"number\">0</span></span><br><span class=\"line\">producer()&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        p(empty);</span><br><span class=\"line\">        p(mutex);</span><br><span class=\"line\">        produce;</span><br><span class=\"line\">       \tv(mutex);</span><br><span class=\"line\">        v(full);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">consumer()&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        p(full);</span><br><span class=\"line\">        p(mutex);</span><br><span class=\"line\">        consume ;</span><br><span class=\"line\">       \tv(mutex);</span><br><span class=\"line\">        v(empty);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<p>一张PPT的图：<br><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240430003954753.png\" alt=\"image-20240430003954753\"></p>\n<h2 id=\"存储管理\"><a href=\"#存储管理\" class=\"headerlink\" title=\"存储管理\"></a>存储管理</h2><h3 id=\"页式存储\"><a href=\"#页式存储\" class=\"headerlink\" title=\"页式存储\"></a>页式存储</h3><p>优点： 利用率高，碎片小，分配及管理简单</p>\n<p>缺点：增加了系统开销，可能产生抖动现象</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240430005646138.png\" alt=\"image-20240430005646138\"></p>\n<h3 id=\"段式存储\"><a href=\"#段式存储\" class=\"headerlink\" title=\"段式存储\"></a>段式存储</h3><p>段式存储主要是，程序段是完整的，不会和页式一样一个程序段被切成多块。</p>\n<p>优点：多道程序共享内存，各段程序修改互不影响。</p>\n<p>缺点：内存利用率低，内存碎片浪费大。</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240503232221396.png\" alt=\"image-20240503232221396\"></p>\n<h3 id=\"段页式存储\"><a href=\"#段页式存储\" class=\"headerlink\" title=\"段页式存储\"></a>段页式存储</h3><p>优点：空间浪费小，存储共享容易，存储保护容易，可以动态链接。</p>\n<p>缺点：由于管理软件的增加，复杂性和开销也增加，性能有所下降</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240503235441587.png\" alt=\"image-20240503235441587\"></p>\n<h2 id=\"虚拟存储器\"><a href=\"#虚拟存储器\" class=\"headerlink\" title=\"虚拟存储器\"></a>虚拟存储器</h2><p>其实实现使用也就是上文的存储管理。</p>\n<p><strong>好像考试也不怎么考这些</strong></p>\n<h3 id=\"页面置换算法\"><a href=\"#页面置换算法\" class=\"headerlink\" title=\"页面置换算法\"></a>页面置换算法</h3><ul>\n<li>先进先出FIFO</li>\n<li>最佳置换optimal</li>\n<li>最近最久未使用LRU</li>\n<li>最少使用LFU</li>\n</ul>\n<h3 id=\"文件组织结构\"><a href=\"#文件组织结构\" class=\"headerlink\" title=\"文件组织结构\"></a>文件组织结构</h3><p>逻辑结构：流式文件，记录式文件。</p>\n<p>物理结构：顺序结构，链接结构，索引结构。</p>\n<p><strong>好像考试也不怎么考这些</strong></p>\n<h3 id=\"虚设备与SPOOLING技术\"><a href=\"#虚设备与SPOOLING技术\" class=\"headerlink\" title=\"虚设备与SPOOLING技术\"></a>虚设备与SPOOLING技术</h3><p>当只有一个物理设备的情况下，有多个用户需要进行使用，所以就需要一个作业井（其实也就理解为一个任务队列）</p>\n<p><strong>随便贴图一张基本上不会考的样子。</strong></p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504112544771.png\" alt=\"image-20240504112544771\"></p>\n<h2 id=\"习题\"><a href=\"#习题\" class=\"headerlink\" title=\"习题\"></a>习题</h2><p>送分题：<br><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240429124040399.png\" alt=\"image-20240429124040399\"></p>\n<p><img src=\"%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240430004454017.png\" alt=\"image-20240430004454017\"></p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504000120592.png\" alt=\"image-20240504000120592\"></p>\n<p>这个题的思路就是，先判断在不在内存里，然后判断有没有访问过，最后判断有没有被修改过。</p>\n<p>实际上就是判断：去掉0访问，去掉0修改。</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504012327600.png\" alt=\"image-20240504012327600\"></p>\n<p>这个地方如果是字，那就是根据cpu的字长来32就是除32.</p>\n<p>如果是字节，那就是除8</p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"计算机硬件基础-cache-校验码","url":"/forward/20906a71.html","content":"<h1 id=\"计算机基础硬件-cache-校验码\"><a href=\"#计算机基础硬件-cache-校验码\" class=\"headerlink\" title=\"计算机基础硬件-cache + 校验码\"></a>计算机基础硬件-cache + 校验码</h1><h2 id=\"cache\"><a href=\"#cache\" class=\"headerlink\" title=\"cache\"></a>cache</h2><p>功能:提高cpu数据输入输出的速率，突破冯诺依曼瓶颈</p>\n<p>速度：在计算机存储体系中，cache是访问速度较快的层次</p>\n<p>原理: 在使用cache改善系统性能的一居室程序的局部性原理。</p>\n<p>组成：cache由控制部分和存储部分组成</p>\n<p><img src=\"/images/pasted-2.png\" alt=\"upload successful\"></p>\n<p><img src=\"/images/pasted-3.png\" alt=\"upload successful\"></p>\n<h2 id=\"输入输出\"><a href=\"#输入输出\" class=\"headerlink\" title=\"输入输出\"></a>输入输出</h2><p><img src=\"/images/pasted-4.png\" alt=\"upload successful\"></p>\n<h2 id=\"校验码-奇偶校验码-，CRC-海明校验码\"><a href=\"#校验码-奇偶校验码-，CRC-海明校验码\" class=\"headerlink\" title=\"校验码-奇偶校验码 ，CRC,海明校验码\"></a>校验码-奇偶校验码 ，CRC,海明校验码</h2><p><img src=\"/images/pasted-5.png\" alt=\"upload successful\"></p>\n<p>奇偶校验码 - 只能检错，可检验1（奇数）位错</p>\n<p>CRC - 只能检错，可检多位</p>\n<p>海明码：可以检错纠错，和1位多位</p>\n<p><img src=\"/images/pasted-6.png\" alt=\"upload successful\"><br>奇偶校验很简单比如：</p>\n<p><span style=\"color:red\">1</span>|00  0 这样子这个就有一个奇数校验</p>\n<p><span style=\"color:red\">0</span>|01      1</p>\n<p><span style=\"color:red\">0</span>|10      2</p>\n<p><span style=\"color:red\">1</span>|11      3</p>\n<p>假如出现一个 <span style=\"color:red\">0</span>|11 这种就在奇数校验下非法了</p>\n<p>CRC 就是k个数据位+r个校验位没啥好说的</p>\n<p>海明码假设由48个数据位</p>\n<p>那么由公式 2^k &gt;= 48 + k     得出k = 6 6个校验位</p>\n<p><img src=\"/images/pasted-7.png\" alt=\"upload successful\"></p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"计算机组成原理指令存储-软考版","url":"/forward/8f372709.html","content":"<h1 id=\"指令-存储软考版本\"><a href=\"#指令-存储软考版本\" class=\"headerlink\" title=\"指令+存储软考版本\"></a>指令+存储软考版本</h1><h2 id=\"指令\"><a href=\"#指令\" class=\"headerlink\" title=\"指令\"></a>指令</h2><p><strong>一条指令就是机器语言的一个语句，是一组有意义的二进制代码</strong></p>\n<p>一条指令其实包含如下内容：”操作码字段” ，”地址码字段”</p>\n<ul>\n<li>操作码字段 - 指出计算机要执行什么性质的操作。</li>\n<li>地址码字段 - 包含各操作数的地址与结果存放地址。</li>\n</ul>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240417225856055.png\" alt=\"image-20240417225856055\"></p>\n<p>如果没有A1 A2 A3只有OP就是0地址指令符</p>\n<h2 id=\"寻址方式\"><a href=\"#寻址方式\" class=\"headerlink\" title=\"寻址方式\"></a>寻址方式</h2><ul>\n<li><p>立即寻址：地址码部分存放的就是操作数</p>\n</li>\n<li><p>直接寻址：地址码存放的是操作数的地址</p>\n</li>\n<li><p>间接寻址：地址码存放的是记录操作数地址的地址。</p>\n</li>\n<li><p>寄存器寻址：地址码部分告诉我们数据存在哪一个寄存器</p>\n</li>\n<li><p>寄存器间接寻址：数据存在哪一个寄存器的地址</p>\n</li>\n<li><p>———————–上面软考常考</p>\n</li>\n<li><p>下面这三个基本上就是加偏移量进行寻址</p>\n</li>\n<li><p>相对寻址-一般个电脑就这个</p>\n</li>\n<li><p>基址寻址</p>\n</li>\n<li><p>变址寻址</p>\n</li>\n</ul>\n<h2 id=\"计算机体系结构分类\"><a href=\"#计算机体系结构分类\" class=\"headerlink\" title=\"计算机体系结构分类\"></a>计算机体系结构分类</h2><table>\n<thead>\n<tr>\n<th>体系结构类型</th>\n<th>结构</th>\n<th>关键特性</th>\n<th>代表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>单指令流，单数据流，<strong>SISD</strong></td>\n<td>控制部分：1<br />处理部件：1</td>\n<td></td>\n<td>单处理器系统</td>\n</tr>\n<tr>\n<td>单指令流，多数据流 <strong>SIMD</strong></td>\n<td>控制部分：1<br />处理部件：多个</td>\n<td>以同步的形式执行同一条指令</td>\n<td>阵列处理机，超级向量处理机</td>\n</tr>\n<tr>\n<td>多指令流，单数据流 <strong>MISD</strong></td>\n<td>控制部分：多个<br />处理部分：1</td>\n<td>不可能且不实际</td>\n<td>目前没有，有点像流水线之类的<br /></td>\n</tr>\n<tr>\n<td>多指令流，多数据流<br /><strong>MSMD</strong></td>\n<td>控制部分：多个<br />处理部分：多个</td>\n<td>能够实现作业任务，指令等各级全面执行</td>\n<td>多处理机系统，多计算机</td>\n</tr>\n</tbody></table>\n<p>阵列处理机：就是多台处理机组成，每台处理机处理相同任务，并行计算。</p>\n<p>多处理机系统：多台处理机设备组成的系统，每台处理机有属于自己的控制部件，可以执行独立的程序，共享一个主存储和所有外部设备。</p>\n<p><img src=\".//images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240418203026838.png\" alt=\"image-20240418203026838\"></p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011317032.png\" alt=\"image-20240424011317032\"></p>\n<h2 id=\"CISC-与-RISC\"><a href=\"#CISC-与-RISC\" class=\"headerlink\" title=\"CISC 与 RISC\"></a>CISC 与 RISC</h2><table>\n<thead>\n<tr>\n<th>–</th>\n<th>CISC（复杂）</th>\n<th>RISC(精简)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>指令</td>\n<td>数量多，使用频率差别大，可变长格式</td>\n<td>数量少<br />使用频率接近<br />定长格式<br />大部分为单周期指令<br />操作寄存器<br />只有Load/Store操作内存</td>\n</tr>\n<tr>\n<td>寻址方式</td>\n<td>支持多种</td>\n<td>支持方式少</td>\n</tr>\n<tr>\n<td>实现方式</td>\n<td>微程序控制技术</td>\n<td>增加了通用寄存器<br />硬布线逻辑控制为主<br />采用流水线<br /></td>\n</tr>\n<tr>\n<td>其他</td>\n<td></td>\n<td>优化编译，有效支持高级语言</td>\n</tr>\n</tbody></table>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011820711.png\" alt=\"image-20240424011820711\"></p>\n<h2 id=\"流水线\"><a href=\"#流水线\" class=\"headerlink\" title=\"流水线\"></a>流水线</h2><p>流水线：多条指令重叠进行操作的一种准并行处理实现技术。</p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011930035.png\" alt=\"image-20240424011930035\"></p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011938087.png\" alt=\"image-20240424011938087\"></p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424012012622.png\" alt=\"image-20240424012012622\"></p>\n<p>上面这个图是不是一下子看不懂？</p>\n<p>根据列题来说，流水线周期其实就是，三部分执行时间中最长的一部分，在如题中也就是2ns。</p>\n<p>那么流水线计算公式呢？</p>\n<p><em><em>单条指令所需时间+（n-1）</em> 流水线周期</em>*</p>\n<p>那么如题就是</p>\n<p>（2+2+1）+ 99 * 2 = 203</p>\n<h2 id=\"多级存储器结构\"><a href=\"#多级存储器结构\" class=\"headerlink\" title=\"多级存储器结构\"></a>多级存储器结构</h2><p>没啥好说的，看图即可，金字塔上面 贵和快和小。下面就是便宜慢和大</p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240418223757692.png\" alt=\"image-20240418223757692\"></p>\n<h2 id=\"存储器分类\"><a href=\"#存储器分类\" class=\"headerlink\" title=\"存储器分类\"></a>存储器分类</h2><p>一般就纠结一些</p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240418223824672.png\" alt=\"image-20240418223824672\"></p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"计算机组成原理部分（基础词汇扫盲+基本组成）-软考版","url":"/forward/624a8c21.html","content":"<h1 id=\"计算机基本组成\"><a href=\"#计算机基本组成\" class=\"headerlink\" title=\"计算机基本组成\"></a>计算机基本组成</h1><p><img src=\"/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.assets/image-20240415190659207.png\" alt=\"image-20240415190659207\"></p>\n<h2 id=\"计算机基组成\"><a href=\"#计算机基组成\" class=\"headerlink\" title=\"计算机基组成\"></a>计算机基组成</h2><ol>\n<li>输入设备</li>\n<li>运算器 控制器 ==》 CPU （其实在这个阶段还有CPU+主存储器被称为主机）还有说法就是CPU其实应该分为（运算器，寄存器组，控制器，内部总线）</li>\n<li>存储器</li>\n<li>输出设备</li>\n</ol>\n<p>其中比较值得记忆的就是<code>运算器</code>和<code>控制器</code></p>\n<h3 id=\"运算器\"><a href=\"#运算器\" class=\"headerlink\" title=\"运算器\"></a>运算器</h3><table>\n<thead>\n<tr>\n<th>模块</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>算数逻辑单元ALU</td>\n<td>进行算式计算和逻辑运算</td>\n</tr>\n<tr>\n<td>累加寄存器（有的设备没有累加寄存器直接用<code>数据缓冲寄存器替代</code>）</td>\n<td>存放数据运算的一个操作数或者结果如31+1=32中的32.也有可能是其中的31。因为它只能存一个。</td>\n</tr>\n<tr>\n<td>数据缓冲寄存器</td>\n<td>保存cpu的运算数和运算结果。<code>这个存的多一些</code></td>\n</tr>\n<tr>\n<td>状态条件寄存器</td>\n<td>在计算机中，它主要用来保存运算过程中的状态信息，比如运算结果、运算过程中的逻辑状态等。当运算出现异区状态时，它能够及时标识出来，帮助计算机更好地进行下一步的运算。</td>\n</tr>\n</tbody></table>\n<h3 id=\"控制器\"><a href=\"#控制器\" class=\"headerlink\" title=\"控制器\"></a>控制器</h3><table>\n<thead>\n<tr>\n<th>模块</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>程序计数器PC</td>\n<td>用于记录下一个需要运行的指令。</td>\n</tr>\n<tr>\n<td>指令寄存器IR</td>\n<td>存放当前运行的任务指令</td>\n</tr>\n<tr>\n<td>指令译码器</td>\n<td>将指令译码为计算机能执行内容</td>\n</tr>\n<tr>\n<td>时序部件</td>\n<td>实际上就是控制cpu 频率的</td>\n</tr>\n</tbody></table>\n<p><img src=\"/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.assets/image-20240415191622335.png\" alt=\"image-20240415191622335\"></p>\n<h2 id=\"并发并行\"><a href=\"#并发并行\" class=\"headerlink\" title=\"并发并行\"></a>并发并行</h2><h3 id=\"并发性\"><a href=\"#并发性\" class=\"headerlink\" title=\"并发性\"></a>并发性</h3><p>并发就是一时间段内运行的任务。</p>\n<h3 id=\"同时性\"><a href=\"#同时性\" class=\"headerlink\" title=\"同时性\"></a>同时性</h3><p>就是同一时刻。</p>\n<h2 id=\"词汇扫盲\"><a href=\"#词汇扫盲\" class=\"headerlink\" title=\"词汇扫盲\"></a>词汇扫盲</h2><p><img src=\"/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.assets/image-20240415191807006.png\" alt=\"image-20240415191807006\"></p>\n<h3 id=\"CPU的性能指标\"><a href=\"#CPU的性能指标\" class=\"headerlink\" title=\"CPU的性能指标\"></a>CPU的性能指标</h3><p>主频就是2.4GHZ这个，</p>\n<p>字长就是一次性能处理的2进制数据长度，如2^64    2^32这样子</p>\n<p>CPU缓存就是L1 L2高速缓存。</p>\n<h3 id=\"总线分类\"><a href=\"#总线分类\" class=\"headerlink\" title=\"总线分类\"></a>总线分类</h3><ul>\n<li>数据总线：顾名思义数据走向的总线。</li>\n<li>控制总线：控制指令总线</li>\n<li>地址总线：内存编址范围。（一般XP系统内存是4G，因为xp的操作系统内存编址是32位 2^32 = 2^2 * 2^30  然后 2^30是一个G，所以算下来就是4G）总之就是管理操作系统运行内存大小</li>\n</ul>\n<p>总线性能：带宽（这个就是是带宽），位宽（一般就是和CPU字长一样，32位就是32位），工作频率（时序频率）</p>\n<p>设备间连接方式：串行连接，和并行连接。 串行线只有一根但是可以很长，并行线可以有多根但是不能很长。前者速度慢但是距离长，后者速度快距离短。</p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"Langchain系列[03]聊天机器人 Chatbot","url":"/forward/53b4c1f8.html","content":"<p>目标</p>\n<p>我们将建立一个带有对话历史的聊天机器人。</p>\n<p>以下是我们将要使用的一些组件：</p>\n<ol>\n<li>聊天模型（Chat Models）：聊天机器人的界面是基于消息传递而不是原始文本，因此它更适合使用聊天模型而不是文本型的 LLM。</li>\n<li>提示模板（Prompt Templates）：这些模板简化了组装提示的过程，可以结合默认消息、用户输入、聊天历史以及（可选的）额外检索到的上下文。</li>\n<li>聊天历史（Chat History）：聊天历史允许聊天机器人“记住”过去的交互，并在回应后续问题时考虑这些历史。</li>\n<li>使用 LangSmith 调试和跟踪你的应用程序（<strong>非必须</strong>）。</li>\n</ol>\n<hr>\n<h3 id=\"聊天历史\"><a href=\"#聊天历史\" class=\"headerlink\" title=\"聊天历史\"></a>聊天历史</h3><p>在[Vol][2]中，我们使用了一个 messages[]来保存和传递消息，它是这个样子</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">      SystemMessage(content=<span class=\"string\">&quot;Translate the following from English into Italian&quot;</span>),</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;hi!&quot;</span>),</span><br><span class=\"line\">      ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>其实，随着对话的增长，我们可以把AI消息和我们自己的消息一起添加到这个列表里</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">      SystemMessage(content=<span class=\"string\">&quot;Translate the following from English into Italian&quot;</span>),</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;hi!&quot;</span>),</span><br><span class=\"line\">      AIMessage(content=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;...&quot;</span>),</span><br><span class=\"line\">      AIMessage(content=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;...&quot;</span>),</span><br><span class=\"line\">      AIMessage(content=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;...&quot;</span>),</span><br><span class=\"line\">      ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>上面[]的内容，就是对话历史…</p>\n<p>我们先不看官方的例子，简单的事非得搞复杂…</p>\n<p>我们先自己维护一下对话历史，感受一下最底层的对话历史逻辑~</p>\n<p>这里我们用通义千问做演示</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用通义千问</span></span><br><span class=\"line\"><span class=\"comment\"># DASHSCOPE_API_KEY 需要在阿里云里面申请相关Key</span></span><br><span class=\"line\">api_key = DASHSCOPE_API_KEY</span><br><span class=\"line\">qwen_chat = ChatOpenAI(</span><br><span class=\"line\">    model_name=<span class=\"string\">&quot;qwen-max&quot;</span>,</span><br><span class=\"line\">    openai_api_base=<span class=\"string\">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class=\"line\">    openai_api_key=api_key,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出解析</span></span><br><span class=\"line\">parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 提示词</span></span><br><span class=\"line\">system_template = <span class=\"string\">&quot;你是一个AI助手。&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成系统提示词 obj</span></span><br><span class=\"line\">sys_msg = SystemMessage(content=system_template)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对话历史，手工添加</span></span><br><span class=\"line\">messages = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 先添加系统消息</span></span><br><span class=\"line\">messages.append(sys_msg)</span><br><span class=\"line\"></span><br><span class=\"line\">res = qwen_chat.invoke(<span class=\"string\">&quot;hi,你好，我是粥~&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加我们的输入</span></span><br><span class=\"line\">messages.append(HumanMessage(content=<span class=\"string\">&quot;hi,你好，我是粥~&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加Ai的回答</span></span><br><span class=\"line\"><span class=\"comment\"># 注意： 大模型输出是一个 AIMessage， 所以我们可以这样添加</span></span><br><span class=\"line\">messages.append(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 我们验证一下，对话历史是否 有效果, 用模型直接根据历史回答</span></span><br><span class=\"line\">messages.append(HumanMessage(content=<span class=\"string\">&quot;我是谁？&quot;</span>))</span><br><span class=\"line\">res = qwen_chat.invoke(messages)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res.content)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>在res = qwen_chat.invoke(messages) 调用前，对话历史是这样的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">   SystemMessage(content=<span class=\"string\">&#x27;你是一个AI助手。&#x27;</span>), </span><br><span class=\"line\">   HumanMessage(content=<span class=\"string\">&#x27;hi,你好，我是粥~&#x27;</span>), </span><br><span class=\"line\">   AIMessage(content=<span class=\"string\">&#x27;你好，粥~！很高兴能与你交流。有什么可以帮助你的吗？&#x27;</span>, response_metadata=&#123;<span class=\"string\">&#x27;token_usage&#x27;</span>: &#123;<span class=\"string\">&#x27;completion_tokens&#x27;</span>: <span class=\"number\">16</span>, <span class=\"string\">&#x27;prompt_tokens&#x27;</span>: <span class=\"number\">15</span>, <span class=\"string\">&#x27;total_tokens&#x27;</span>: <span class=\"number\">31</span>&#125;, <span class=\"string\">&#x27;model_name&#x27;</span>: <span class=\"string\">&#x27;qwen-max&#x27;</span>, <span class=\"string\">&#x27;system_fingerprint&#x27;</span>: <span class=\"literal\">None</span>, <span class=\"string\">&#x27;finish_reason&#x27;</span>: <span class=\"string\">&#x27;stop&#x27;</span>, <span class=\"string\">&#x27;logprobs&#x27;</span>: <span class=\"literal\">None</span>&#125;, <span class=\"built_in\">id</span>=<span class=\"string\">&#x27;run-ae2d37f0-287f-4c37-a0cd-bc0a8cc2114f-0&#x27;</span>), </span><br><span class=\"line\">   HumanMessage(content=<span class=\"string\">&#x27;我是谁？&#x27;</span>)</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>AI最后的回答</p>\n<p>可以看到，AI还记得我是粥<del>，这个效果是因为我们在messages里有对话历史</del></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">content=<span class=\"string\">&#x27;你是卷儿，刚刚你自己介绍过了哦。如果有什么想聊的或者需要帮助的，尽管告诉我！&#x27;</span> response_metadata=&#123;<span class=\"string\">&#x27;token_usage&#x27;</span>: &#123;<span class=\"string\">&#x27;completion_tokens&#x27;</span>: <span class=\"number\">23</span>, <span class=\"string\">&#x27;prompt_tokens&#x27;</span>: <span class=\"number\">54</span>, <span class=\"string\">&#x27;total_tokens&#x27;</span>: <span class=\"number\">77</span>&#125;, <span class=\"string\">&#x27;model_name&#x27;</span>: <span class=\"string\">&#x27;qwen-max&#x27;</span>, <span class=\"string\">&#x27;system_fingerprint&#x27;</span>: <span class=\"literal\">None</span>, <span class=\"string\">&#x27;finish_reason&#x27;</span>: <span class=\"string\">&#x27;stop&#x27;</span>, <span class=\"string\">&#x27;logprobs&#x27;</span>: <span class=\"literal\">None</span>&#125; <span class=\"built_in\">id</span>=<span class=\"string\">&#x27;run-979eb2bb-fa93-4d18-bda0-288d6d0a11b5-0&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Message-History\"><a href=\"#Message-History\" class=\"headerlink\" title=\"Message History\"></a>Message History</h3><p>这是langchain 实现的一个对话历史的方法，不是那么重要，但是其中提到的 <strong>通过config结构获取对话历史</strong>，这种</p>\n<p>模式比较重要~</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在代码中讲解</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_message_histories <span class=\"keyword\">import</span> ChatMessageHistory</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.chat_history <span class=\"keyword\">import</span> BaseChatMessageHistory</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables.history <span class=\"keyword\">import</span> RunnableWithMessageHistory</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 通过ID 返回一个ChatMessageHistory</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_session_history</span>(<span class=\"params\">session_id: <span class=\"built_in\">str</span></span>) -&gt; BaseChatMessageHistory:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> session_id <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> store:</span><br><span class=\"line\">        store[session_id] = ChatMessageHistory()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> store[session_id]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用通义千问</span></span><br><span class=\"line\">model = qwen_chat</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># langchain 的RunnableWithMessageHistory</span></span><br><span class=\"line\"><span class=\"comment\"># 需要传递一个runnable，一个 获得历史的方法 ：get_session_history</span></span><br><span class=\"line\">with_message_history = RunnableWithMessageHistory(model, get_session_history)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># session_id 是一个 TAG， 一个记录，表示不同的配置，比如你有很多对话，用不同的tag 表示不同的对话历史</span></span><br><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 自动保存历史，不用我们手动保存了</span></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;Hi! I&#x27;m Bob&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 同样的config， 同样的对话历史，AI记得我们是谁</span></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用不同TAG，abc3， 即不同的历史，AI不记得我们是谁</span></span><br><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc3&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用之间的TAG，就可以识别了~</span></span><br><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br></pre></td></tr></table></figure>\n\n<p><strong>Streaming</strong></p>\n<p>langchain提供了很多方法， 流式输出是其中的一个例子，还包括 批处理（batch）,异步调用 等等。</p>\n<p>一个chain 定义之后，我们想用哪种方法调用都可以，只需要换一个方法即可~</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 流式输出</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> with_message_history.stream([HumanMessage(content=<span class=\"string\">&quot;hi! tell me a joke&quot;</span>)],</span><br><span class=\"line\">       config=config,</span><br><span class=\"line\">):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(r.content, end=<span class=\"string\">&quot;|&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p><strong>输出</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">|Sure|,| here|<span class=\"string\">&#x27;s a joke for you|, Bob: &lt;--AI 还记得我们</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">Why don&#x27;</span>t scientists trust| atoms?</span><br><span class=\"line\"></span><br><span class=\"line\">Because they make up everything!||</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"参考代码\"><a href=\"#参考代码\" class=\"headerlink\" title=\"参考代码\"></a>参考代码</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables <span class=\"keyword\">import</span> RunnablePassthrough</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_openai <span class=\"keyword\">import</span> AzureChatOpenAI, ChatOpenAI</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.messages <span class=\"keyword\">import</span> HumanMessage, SystemMessage</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> llm_cfg <span class=\"keyword\">import</span> AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, DEPLOYMENT_NAME_GPT3P5, MY_QIANFAN_AK, MY_QIANFAN_SK, \\</span><br><span class=\"line\">    DASHSCOPE_API_KEY</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用 通义千问</span></span><br><span class=\"line\">    api_key = DASHSCOPE_API_KEY</span><br><span class=\"line\">    qwen_chat = ChatOpenAI(</span><br><span class=\"line\">        model_name=<span class=\"string\">&quot;qwen-max&quot;</span>,</span><br><span class=\"line\">        openai_api_base=<span class=\"string\">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class=\"line\">        openai_api_key=api_key,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用 Azure OpenAi</span></span><br><span class=\"line\">    <span class=\"comment\"># os.environ[&quot;AZURE_OPENAI_API_KEY&quot;] =AZURE_OPENAI_API_KEY</span></span><br><span class=\"line\">    <span class=\"comment\"># os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;] =AZURE_OPENAI_ENDPOINT</span></span><br><span class=\"line\">    <span class=\"comment\"># gpt3p5_model = AzureChatOpenAI(</span></span><br><span class=\"line\">    <span class=\"comment\">#     openai_api_version=&quot;2024-02-15-preview&quot;,</span></span><br><span class=\"line\">    <span class=\"comment\">#     azure_deployment=DEPLOYMENT_NAME_GPT3P5,</span></span><br><span class=\"line\">    <span class=\"comment\"># )</span></span><br><span class=\"line\"></span><br><span class=\"line\">    parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词</span></span><br><span class=\"line\">    system_template = <span class=\"string\">&quot;你是一个AI助手。&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 生成系统提示词 obj</span></span><br><span class=\"line\">    sys_msg = SystemMessage(content=system_template)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 对话历史，手工添加</span></span><br><span class=\"line\">    messages = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 先添加系统消息</span></span><br><span class=\"line\">    messages.append(sys_msg)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># res = qwen_chat.invoke(&quot;hi,你好，我是卷儿&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># print(res)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 添加我们的输入</span></span><br><span class=\"line\">    messages.append(HumanMessage(content=<span class=\"string\">&quot;hi,你好，我是卷儿&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 添加Ai的回答</span></span><br><span class=\"line\">    <span class=\"comment\"># 注意： 大模型输出是一个 AIMessage， 所以我们可以这样添加</span></span><br><span class=\"line\">    <span class=\"comment\"># messages.append(res)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 我们验证一下，对话历史是否 有效果, 用模型直接根据历史回答</span></span><br><span class=\"line\">    messages.append(HumanMessage(content=<span class=\"string\">&quot;我是谁？&quot;</span>))</span><br><span class=\"line\">    <span class=\"comment\"># res = qwen_chat.invoke(messages)</span></span><br><span class=\"line\">    <span class=\"comment\"># print(res.content)</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    store = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#Message History</span></span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_community.chat_message_histories <span class=\"keyword\">import</span> ChatMessageHistory</span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_core.chat_history <span class=\"keyword\">import</span> BaseChatMessageHistory</span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_core.runnables.history <span class=\"keyword\">import</span> RunnableWithMessageHistory</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">get_session_history</span>(<span class=\"params\">session_id: <span class=\"built_in\">str</span></span>) -&gt; BaseChatMessageHistory:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> session_id <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> store:</span><br><span class=\"line\">            store[session_id] = ChatMessageHistory()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> store[session_id]</span><br><span class=\"line\"></span><br><span class=\"line\">    model = qwen_chat</span><br><span class=\"line\">    with_message_history = RunnableWithMessageHistory(model, get_session_history)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># session_id 是一个 TAG， 一个记录，表示不同的配置，比如你有很多对话，用不同的tag 表示不同的对话历史</span></span><br><span class=\"line\">    config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;Hi! I&#x27;m Bob&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 同样的config， 同样的对话历史</span></span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用不同TAG，abc3， 即不同的历史</span></span><br><span class=\"line\">    config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc3&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 不认识之前提到的名字</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 用之间的TAG，就可以识别了~</span></span><br><span class=\"line\">    config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> with_message_history.stream([HumanMessage(content=<span class=\"string\">&quot;hi! tell me a joke&quot;</span>)],</span><br><span class=\"line\">           config=config,</span><br><span class=\"line\">    ):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(r.content, end=<span class=\"string\">&quot;|&quot;</span>)</span><br></pre></td></tr></table></figure>","tags":["langchain"]}]