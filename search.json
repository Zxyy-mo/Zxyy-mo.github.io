[{"title":"LLama2大模型量化部署","url":"/forward/1f77414f.html","content":"<h1 id=\"Llama模型量化模型cpp部署\"><a href=\"#Llama模型量化模型cpp部署\" class=\"headerlink\" title=\"Llama模型量化模型cpp部署\"></a>Llama模型量化模型cpp部署</h1><h2 id=\"部署步骤\"><a href=\"#部署步骤\" class=\"headerlink\" title=\"部署步骤\"></a>部署步骤</h2><p>wsl下部署没啥装个Ubuntu22.04先。</p>\n<p>然后git clone两个项目。</p>\n<p>首先是llama的git项目</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/facebookresearch/llama.git</span><br></pre></td></tr></table></figure>\n\n<p>然后是cpp</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br></pre></td></tr></table></figure>\n\n<p>然后进入llama的项目</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> llama</span><br></pre></td></tr></table></figure>\n\n<p>然后去官网<a href=\"https://ai.meta.com/resources/models-and-libraries/llama-downloads/\">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p>\n<p>申请下载到邮箱</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./download.sh</span><br></pre></td></tr></table></figure>\n\n<p>然后就是这样子</p>\n<p><img src=\"/images/pasted-1.png\" alt=\"upload successful\"></p>\n<p>等待模型下载完毕以后，进入llama.cpp</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> llama.cpp</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>先安装gcc环境</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">sudo apt install build-essential</span><br></pre></td></tr></table></figure>\n\n<p>编译等待编译完成</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">make</span><br></pre></td></tr></table></figure>\n\n<p>安装python依赖</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">python3 -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure>\n\n<p>转换模型，当然还有其他参数我们可以直接打开convert.py去查看</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">python3 convert.py <span class=\"string\">&#x27;模型地址&#x27;</span></span><br><span class=\"line\">python3 convert.py --outfile <span class=\"string\">&#x27;输出地址&#x27;</span> <span class=\"string\">&#x27;模型地址&#x27;</span></span><br><span class=\"line\">python3 convert.py --outfile ./models/llama-2-7b-chat ../llama/llama-2-7b-chat/</span><br></pre></td></tr></table></figure>\n\n<p>那么如果遇到以下问题</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">```bash</span><br><span class=\"line\">python3 haConvert.py --outfile ./models/llama-2-7b-chat ../llama/llama-2-7b-chat/</span><br><span class=\"line\">```</span><br><span class=\"line\">```error</span><br><span class=\"line\">Writing models/llama-2-7b-chat, format 1</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 1210, <span class=\"keyword\">in</span> &lt;module&gt;</span><br><span class=\"line\">    main()</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 1205, <span class=\"keyword\">in</span> main</span><br><span class=\"line\">    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 909, <span class=\"keyword\">in</span> write_all</span><br><span class=\"line\">    check_vocab_size(params, vocab)</span><br><span class=\"line\">  File <span class=\"string\">&quot;/home/xiaoyu/llama.cpp/haConvert.py&quot;</span>, line 796, <span class=\"keyword\">in</span> check_vocab_size</span><br><span class=\"line\">    raise Exception(msg)</span><br><span class=\"line\">Exception: Vocab size mismatch (model has -1, but ../llama/tokenizer.model has 32000).</span><br><span class=\"line\">```</span><br></pre></td></tr></table></figure>\n\n<p>不要去相信网上的added_tokens.json</p>\n<p>实际上在llama-2-7b-chat文件夹中，应该有一个.json文件（可能是params.json）。打开这个json文件，将”vocab_size”从-1改为32000。</p>\n<p><img src=\"/images/pasted-0.png\" alt=\"upload successful\"></p>\n<h2 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h2><p>实际上最后输出的是一个.bin文件，所以我们上面的命令是有瑕疵的。</p>\n<p>所以当我们转换成功以后。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> models</span><br><span class=\"line\"><span class=\"built_in\">mkdir</span> 7B</span><br><span class=\"line\"><span class=\"built_in\">mv</span> llama-2-7b-chat 7B/ggml-model-f16.bin</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"量化\"><a href=\"#量化\" class=\"headerlink\" title=\"量化\"></a>量化</h2><p>我们上面改为了bin这里也变成bin，我们执行的是4bit量化, 输出到./models/7B/目录下面</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.gguf q4_0</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h2><figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">./main -m ./models/7B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r <span class=\"string\">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure>","categories":["探索"],"tags":["大模型应用"]},{"title":"Langchain系列[01]介绍","url":"/forward/efdbd4e2.html","content":"<p>大家好，这里是<strong>粥余</strong>。<br>随着大模型技术的飞速发展，**<code>langchain</code>** 的迭代也来到了<strong>2.0</strong> 时代。<br>按照Langchain 新的文档结构再结合之前的资料，我们重新来整理一下相关知识。</p>\n<hr>\n<p>先来看下官网介绍： <a href=\"https://python.langchain.com/v0.2/docs/introduction/\">传送门</a></p>\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p><strong>LangChain</strong> 是一个开发由大型语言模型（LLMs）驱动的应用程序的框架。</p>\n<p>LangChain 简化了 LLM 应用生命周期的每一个阶段：</p>\n<ul>\n<li><strong>开发</strong>：使用 LangChain 的开源 <a href=\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\">构建块</a> 和 <a href=\"https://python.langchain.com/v0.2/docs/concepts/\">组件</a> 构建您的应用程序。利用 <a href=\"https://python.langchain.com/v0.2/docs/integrations/platforms/\">第三方集成</a> 和 <a href=\"https://python.langchain.com/v0.2/docs/templates/\">模板</a> 快速上手。</li>\n<li><strong>生产化</strong>：使用 <a href=\"https://docs.smith.langchain.com/\">LangSmith</a> 来检查、监控和评估您的链，以便您可以持续优化并自信地部署。</li>\n<li><strong>部署</strong>：使用 <a href=\"https://python.langchain.com/v0.2/docs/langserve/\">LangServe</a> 将任何链转变为 API。</li>\n</ul>\n<p><img src=\"/images/Langchain%E7%B3%BB%E5%88%97-01-%E4%BB%8B%E7%BB%8D.assets/image-20240627095511759.png\" alt=\"image-20240627095511759\"></p>\n<p>具体来说，该框架包括以下开源库：</p>\n<ul>\n<li>**<code>langchain-core</code>**：基本抽象和 LangChain 表达式语言。</li>\n<li>**<code>langchain-community</code>**：第三方集成。</li>\n<li>**<code>langchain</code>**：构成应用程序认知架构的链、代理和检索策略。</li>\n<li>**<a href=\"https://langchain-ai.github.io/langgraph\">langgraph</a>**：通过将步骤建模为图中的边和节点，使用 LLMs 构建健壮且具有状态的多参与者应用程序。</li>\n<li>**<a href=\"https://python.langchain.com/v0.2/docs/langserve/\">langserve</a>**：将 LangChain 链作为 REST API 部署。</li>\n</ul>\n<hr>\n<p>咱们先简单的概括一下：</p>\n<p><a href=\"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\">构建块</a></p>\n<p>就是LangChain Expression Language (LCEL)， 也就是链中的各个组件:</p>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Input Type</th>\n<th>Output Type</th>\n<th></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Prompt</td>\n<td>Dictionary</td>\n<td>PromptValue</td>\n<td>提示词</td>\n</tr>\n<tr>\n<td>ChatModel</td>\n<td>Single string, list of chat messages or a PromptValue</td>\n<td>ChatMessage</td>\n<td>chat 模型（支持多轮）</td>\n</tr>\n<tr>\n<td>LLM</td>\n<td>Single string, list of chat messages or a PromptValue</td>\n<td>String</td>\n<td>completion （单轮）</td>\n</tr>\n<tr>\n<td>OutputParser</td>\n<td>The output of an LLM or ChatModel</td>\n<td>Depends on the parser</td>\n<td>输出解析器</td>\n</tr>\n<tr>\n<td>Retriever</td>\n<td>Single string</td>\n<td>List of Documents</td>\n<td>检索器</td>\n</tr>\n<tr>\n<td>Tool</td>\n<td>Single string or dictionary, depending on the tool</td>\n<td>Depends on the tool</td>\n<td>工具</td>\n</tr>\n</tbody></table>\n<p><strong>提示词</strong>： 很好理解，给模型的指令，完成某个任务，比如让模型写一首诗。</p>\n<p><strong>ChatModel</strong> ：使用消息序列作为输入并返回聊天消息作为输出（而不是使用纯文本）的语言模型。聊天模型支持为对话消息分配不同的角色，帮助区分来自AI、用户以及系统消息等指令的消息。用户和AI对话，支持聊天历史，AI有记忆。</p>\n<p><strong>LLM</strong>:以字符串作为输入并返回字符串的语言模型。AI没有记忆，通常是一锤子买卖，用于完成一次性任务。比如翻译一句话。</p>\n<p>注意：</p>\n<p><strong><code>ChatModel</code></strong> 类似于 <strong>OpenAI Chat</strong></p>\n<p><strong><code>LLM</code></strong> 类似于 <strong>OpenAI Completions</strong></p>\n<p><strong>Chat vs Completions</strong></p>\n<p>OpenAI的ChatCompletion和Completion都是自然语言生成模型的接口，但它们的用途和应用场景略有不同。</p>\n<h4 id=\"Completions\"><a href=\"#Completions\" class=\"headerlink\" title=\"Completions\"></a>Completions</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">通用的自然语言生成接口，支持生成各种类型的文本，包括段落、摘要、建议、答案等等。</span><br><span class=\"line\"></span><br><span class=\"line\">Completion接口的输出更为多样化，可能会更加严谨和专业，适用于各种文本生成场景，例如文章创作、信息提取、机器翻译、自然语言问题回答等等。</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Chat\"><a href=\"#Chat\" class=\"headerlink\" title=\"Chat\"></a>Chat</h4><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">专为生成对话和聊天场景而设计。</span><br><span class=\"line\"></span><br><span class=\"line\">ChatCompletion接口生成的文本通常会更具有人类对话的风格和语调，可以用于智能客服、聊天机器人等场景，以及在日常聊天中帮助用户自动生成回复。</span><br></pre></td></tr></table></figure>\n\n<p><strong>输出解析器</strong>：处理大模型的输出，可以将输出转化成需要的格式，比如转化成特定的JSON格式，便于后续流程处理。</p>\n<p><strong>检索器</strong>：在langchain中，检索器通常和<strong>向量数据库</strong>、<strong>嵌入模型</strong>绑定，用于语义搜索。比如可以在本地向量数据库Chroma 中使用BGE-large的中文模型，对中文数据进行相似度搜索。</p>\n<p>工具： 通常是我们自己定义的，能够实现某种功能的方法。工具可以被大模型识别，当用户的需求需要 调用工具时，大模型会自动识别，并让我们调用工具。Langchain 中的Agent 可以自己识别并调用用户定义的工具。</p>\n<hr>\n<p><a href=\"https://python.langchain.com/v0.2/docs/concepts/\">组件</a>：</p>\n<p>这里的组件，是指langchain 工具包含的各个重要的“包”。主要是下面这个几个包：</p>\n<p><strong>langchain-core</strong></p>\n<p>这个包包含了不同组件的基础抽象以及如何将它们组合在一起的方法。在这里定义了LLMs、vectorstores、retrievers等核心组件的接口。</p>\n<p><strong>langchain</strong></p>\n<p>主要的langchain包，包含了构成应用程序认知架构的链、代理和检索策略。这里的所有链、代理和检索策略都不是特定于任何一种集成，而是适用于所有集成的通用类型。</p>\n<p><strong>langchain-community</strong></p>\n<p>这个包包含了由LangChain社区维护的第三方集成。关键的合作伙伴包已经被分离出去。这个包包含了各种组件（LLMs、vectorstores、retrievers）的所有集成。为了尽可能保持包的轻量级，这个包中的所有依赖关系都是可选的。各个模型厂商、向量数据库厂商的包，都在这里。</p>\n<p><strong>LangGraph</strong></p>\n<p>langgraph是langchain的一个扩展，旨在通过将步骤建模为图中的边和节点，使用LLMs构建健壮且具有状态的多参与者应用程序。</p>\n<p>提供了创建常见类型代理的<strong>高级接口</strong>，以及用于组合自定义流程的API。</p>\n<p><strong>也是在langchain中构建复杂Agent 逻辑的基础。</strong>（Langchain 的部分现成的Agent 就是用LangGraph 实现）</p>\n<p><strong>langserve</strong></p>\n<p>一个用于将LangChain链作为REST APIs部署的包。它使得快速启动并运行一个生产就绪的API变得简单。能够让我们快速搭建AI应用的前后端。</p>\n<p><strong>LangSmith</strong></p>\n<p>一个开发者平台，可以调试、测试、评估和监控LLM应用程序。</p>\n<hr>\n<p><a href=\"https://python.langchain.com/v0.2/docs/integrations/platforms/\">第三方集成</a> ：</p>\n<p>只要不是langchain 自己开发的，包括 大语言模型、嵌入模型、向量数据库、第三方工具等等，都属于第三方集成。</p>\n<p><a href=\"https://python.langchain.com/v0.2/docs/templates/\">模板</a>：</p>\n<p>langchain 提供的一些可以开箱体验的功能，类似于脚手架，安装之后可以立即体验。</p>\n<hr>\n<p>在正式接触langchain 之前，小伙伴们可以先到国内大厂的网站注册一下，Langchain官方的代码里</p>\n<p>国内模型作为例子的情况不多，我们主要使用国内的模型给大家介绍，小伙伴们需要先去注册一下。</p>\n<p>国内比较不错的模型（性价比高，部分模型免费）：</p>\n<p><strong>百度千帆大模型平台</strong> ： <a href=\"https://qianfan.cloud.baidu.com/\">传送门</a></p>\n<ul>\n<li>ERNIE 4.0 文心一言4</li>\n<li>ERNIE 3.5 文心一言3.5</li>\n<li>ERNIE Speed <strong>免费</strong></li>\n<li>ERNIE Lite <strong>免费</strong></li>\n<li>ERNIE Tiny <strong>免费</strong></li>\n</ul>\n<p><strong>智谱AI</strong> ： <a href=\"https://open.bigmodel.cn/\">传送门</a></p>\n<ul>\n<li>GLM-4-0520</li>\n<li>GLM-4-AirX</li>\n<li>GLM-4-Air</li>\n</ul>\n<p>**通义千问 （**DashScope灵积模型服务**）**： <a href=\"https://help.aliyun.com/zh/dashscope/developer-reference/api-details\">传送门</a></p>\n<ul>\n<li>qwen-turbo</li>\n<li>qwen-plus</li>\n<li>qwen-max</li>\n</ul>\n<p><strong>字节跳动 豆包模型</strong>： <a href=\"https://www.volcengine.com/product/doubao\">传送门</a></p>\n<ul>\n<li>Doubao-lite-4k</li>\n<li>Doubao-pro-32k</li>\n</ul>\n<p><strong>Moonshot</strong>：<a href=\"https://platform.moonshot.cn/docs/intro#%E4%B8%BB%E8%A6%81%E6%A6%82%E5%BF%B5\"> 传送门</a></p>\n<ul>\n<li>moonshot-v1-128k</li>\n<li>moonshot-v1-32k</li>\n</ul>\n","tags":["langchain"]},{"title":"Langchain系列[03]聊天机器人 Chatbot","url":"/forward/53b4c1f8.html","content":"<p>目标</p>\n<p>我们将建立一个带有对话历史的聊天机器人。</p>\n<p>以下是我们将要使用的一些组件：</p>\n<ol>\n<li>聊天模型（Chat Models）：聊天机器人的界面是基于消息传递而不是原始文本，因此它更适合使用聊天模型而不是文本型的 LLM。</li>\n<li>提示模板（Prompt Templates）：这些模板简化了组装提示的过程，可以结合默认消息、用户输入、聊天历史以及（可选的）额外检索到的上下文。</li>\n<li>聊天历史（Chat History）：聊天历史允许聊天机器人“记住”过去的交互，并在回应后续问题时考虑这些历史。</li>\n<li>使用 LangSmith 调试和跟踪你的应用程序（<strong>非必须</strong>）。</li>\n</ol>\n<hr>\n<h3 id=\"聊天历史\"><a href=\"#聊天历史\" class=\"headerlink\" title=\"聊天历史\"></a>聊天历史</h3><p>在[Vol][2]中，我们使用了一个 messages[]来保存和传递消息，它是这个样子</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">      SystemMessage(content=<span class=\"string\">&quot;Translate the following from English into Italian&quot;</span>),</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;hi!&quot;</span>),</span><br><span class=\"line\">      ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>其实，随着对话的增长，我们可以把AI消息和我们自己的消息一起添加到这个列表里</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">      SystemMessage(content=<span class=\"string\">&quot;Translate the following from English into Italian&quot;</span>),</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;hi!&quot;</span>),</span><br><span class=\"line\">      AIMessage(content=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;...&quot;</span>),</span><br><span class=\"line\">      AIMessage(content=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;...&quot;</span>),</span><br><span class=\"line\">      AIMessage(content=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\">      HumanMessage(content=<span class=\"string\">&quot;...&quot;</span>),</span><br><span class=\"line\">      ...</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>上面[]的内容，就是对话历史…</p>\n<p>我们先不看官方的例子，简单的事非得搞复杂…</p>\n<p>我们先自己维护一下对话历史，感受一下最底层的对话历史逻辑~</p>\n<p>这里我们用通义千问做演示</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用通义千问</span></span><br><span class=\"line\"><span class=\"comment\"># DASHSCOPE_API_KEY 需要在阿里云里面申请相关Key</span></span><br><span class=\"line\">api_key = DASHSCOPE_API_KEY</span><br><span class=\"line\">qwen_chat = ChatOpenAI(</span><br><span class=\"line\">    model_name=<span class=\"string\">&quot;qwen-max&quot;</span>,</span><br><span class=\"line\">    openai_api_base=<span class=\"string\">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class=\"line\">    openai_api_key=api_key,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出解析</span></span><br><span class=\"line\">parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 提示词</span></span><br><span class=\"line\">system_template = <span class=\"string\">&quot;你是一个AI助手。&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 生成系统提示词 obj</span></span><br><span class=\"line\">sys_msg = SystemMessage(content=system_template)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对话历史，手工添加</span></span><br><span class=\"line\">messages = []</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 先添加系统消息</span></span><br><span class=\"line\">messages.append(sys_msg)</span><br><span class=\"line\"></span><br><span class=\"line\">res = qwen_chat.invoke(<span class=\"string\">&quot;hi,你好，我是粥~&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加我们的输入</span></span><br><span class=\"line\">messages.append(HumanMessage(content=<span class=\"string\">&quot;hi,你好，我是粥~&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加Ai的回答</span></span><br><span class=\"line\"><span class=\"comment\"># 注意： 大模型输出是一个 AIMessage， 所以我们可以这样添加</span></span><br><span class=\"line\">messages.append(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 我们验证一下，对话历史是否 有效果, 用模型直接根据历史回答</span></span><br><span class=\"line\">messages.append(HumanMessage(content=<span class=\"string\">&quot;我是谁？&quot;</span>))</span><br><span class=\"line\">res = qwen_chat.invoke(messages)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res.content)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>在res = qwen_chat.invoke(messages) 调用前，对话历史是这样的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">[</span><br><span class=\"line\">   SystemMessage(content=<span class=\"string\">&#x27;你是一个AI助手。&#x27;</span>), </span><br><span class=\"line\">   HumanMessage(content=<span class=\"string\">&#x27;hi,你好，我是粥~&#x27;</span>), </span><br><span class=\"line\">   AIMessage(content=<span class=\"string\">&#x27;你好，粥~！很高兴能与你交流。有什么可以帮助你的吗？&#x27;</span>, response_metadata=&#123;<span class=\"string\">&#x27;token_usage&#x27;</span>: &#123;<span class=\"string\">&#x27;completion_tokens&#x27;</span>: <span class=\"number\">16</span>, <span class=\"string\">&#x27;prompt_tokens&#x27;</span>: <span class=\"number\">15</span>, <span class=\"string\">&#x27;total_tokens&#x27;</span>: <span class=\"number\">31</span>&#125;, <span class=\"string\">&#x27;model_name&#x27;</span>: <span class=\"string\">&#x27;qwen-max&#x27;</span>, <span class=\"string\">&#x27;system_fingerprint&#x27;</span>: <span class=\"literal\">None</span>, <span class=\"string\">&#x27;finish_reason&#x27;</span>: <span class=\"string\">&#x27;stop&#x27;</span>, <span class=\"string\">&#x27;logprobs&#x27;</span>: <span class=\"literal\">None</span>&#125;, <span class=\"built_in\">id</span>=<span class=\"string\">&#x27;run-ae2d37f0-287f-4c37-a0cd-bc0a8cc2114f-0&#x27;</span>), </span><br><span class=\"line\">   HumanMessage(content=<span class=\"string\">&#x27;我是谁？&#x27;</span>)</span><br><span class=\"line\">]</span><br></pre></td></tr></table></figure>\n\n<p>AI最后的回答</p>\n<p>可以看到，AI还记得我是粥<del>，这个效果是因为我们在messages里有对话历史</del></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">content=<span class=\"string\">&#x27;你是卷儿，刚刚你自己介绍过了哦。如果有什么想聊的或者需要帮助的，尽管告诉我！&#x27;</span> response_metadata=&#123;<span class=\"string\">&#x27;token_usage&#x27;</span>: &#123;<span class=\"string\">&#x27;completion_tokens&#x27;</span>: <span class=\"number\">23</span>, <span class=\"string\">&#x27;prompt_tokens&#x27;</span>: <span class=\"number\">54</span>, <span class=\"string\">&#x27;total_tokens&#x27;</span>: <span class=\"number\">77</span>&#125;, <span class=\"string\">&#x27;model_name&#x27;</span>: <span class=\"string\">&#x27;qwen-max&#x27;</span>, <span class=\"string\">&#x27;system_fingerprint&#x27;</span>: <span class=\"literal\">None</span>, <span class=\"string\">&#x27;finish_reason&#x27;</span>: <span class=\"string\">&#x27;stop&#x27;</span>, <span class=\"string\">&#x27;logprobs&#x27;</span>: <span class=\"literal\">None</span>&#125; <span class=\"built_in\">id</span>=<span class=\"string\">&#x27;run-979eb2bb-fa93-4d18-bda0-288d6d0a11b5-0&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Message-History\"><a href=\"#Message-History\" class=\"headerlink\" title=\"Message History\"></a>Message History</h3><p>这是langchain 实现的一个对话历史的方法，不是那么重要，但是其中提到的 <strong>通过config结构获取对话历史</strong>，这种</p>\n<p>模式比较重要~</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br></pre></td></tr></table></figure>\n\n<p>在代码中讲解</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_message_histories <span class=\"keyword\">import</span> ChatMessageHistory</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.chat_history <span class=\"keyword\">import</span> BaseChatMessageHistory</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables.history <span class=\"keyword\">import</span> RunnableWithMessageHistory</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 通过ID 返回一个ChatMessageHistory</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_session_history</span>(<span class=\"params\">session_id: <span class=\"built_in\">str</span></span>) -&gt; BaseChatMessageHistory:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> session_id <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> store:</span><br><span class=\"line\">        store[session_id] = ChatMessageHistory()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> store[session_id]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用通义千问</span></span><br><span class=\"line\">model = qwen_chat</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># langchain 的RunnableWithMessageHistory</span></span><br><span class=\"line\"><span class=\"comment\"># 需要传递一个runnable，一个 获得历史的方法 ：get_session_history</span></span><br><span class=\"line\">with_message_history = RunnableWithMessageHistory(model, get_session_history)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># session_id 是一个 TAG， 一个记录，表示不同的配置，比如你有很多对话，用不同的tag 表示不同的对话历史</span></span><br><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 自动保存历史，不用我们手动保存了</span></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;Hi! I&#x27;m Bob&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 同样的config， 同样的对话历史，AI记得我们是谁</span></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用不同TAG，abc3， 即不同的历史，AI不记得我们是谁</span></span><br><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc3&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用之间的TAG，就可以识别了~</span></span><br><span class=\"line\">config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">response = with_message_history.invoke(</span><br><span class=\"line\">    [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">    config=config,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(response.content)</span><br></pre></td></tr></table></figure>\n\n<p><strong>Streaming</strong></p>\n<p>langchain提供了很多方法， 流式输出是其中的一个例子，还包括 批处理（batch）,异步调用 等等。</p>\n<p>一个chain 定义之后，我们想用哪种方法调用都可以，只需要换一个方法即可~</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 流式输出</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> with_message_history.stream([HumanMessage(content=<span class=\"string\">&quot;hi! tell me a joke&quot;</span>)],</span><br><span class=\"line\">       config=config,</span><br><span class=\"line\">):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(r.content, end=<span class=\"string\">&quot;|&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p><strong>输出</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">|Sure|,| here|<span class=\"string\">&#x27;s a joke for you|, Bob: &lt;--AI 还记得我们</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">Why don&#x27;</span>t scientists trust| atoms?</span><br><span class=\"line\"></span><br><span class=\"line\">Because they make up everything!||</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"参考代码\"><a href=\"#参考代码\" class=\"headerlink\" title=\"参考代码\"></a>参考代码</h3><figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables <span class=\"keyword\">import</span> RunnablePassthrough</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_openai <span class=\"keyword\">import</span> AzureChatOpenAI, ChatOpenAI</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.messages <span class=\"keyword\">import</span> HumanMessage, SystemMessage</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> llm_cfg <span class=\"keyword\">import</span> AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, DEPLOYMENT_NAME_GPT3P5, MY_QIANFAN_AK, MY_QIANFAN_SK, \\</span><br><span class=\"line\">    DASHSCOPE_API_KEY</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用 通义千问</span></span><br><span class=\"line\">    api_key = DASHSCOPE_API_KEY</span><br><span class=\"line\">    qwen_chat = ChatOpenAI(</span><br><span class=\"line\">        model_name=<span class=\"string\">&quot;qwen-max&quot;</span>,</span><br><span class=\"line\">        openai_api_base=<span class=\"string\">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class=\"line\">        openai_api_key=api_key,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用 Azure OpenAi</span></span><br><span class=\"line\">    <span class=\"comment\"># os.environ[&quot;AZURE_OPENAI_API_KEY&quot;] =AZURE_OPENAI_API_KEY</span></span><br><span class=\"line\">    <span class=\"comment\"># os.environ[&quot;AZURE_OPENAI_ENDPOINT&quot;] =AZURE_OPENAI_ENDPOINT</span></span><br><span class=\"line\">    <span class=\"comment\"># gpt3p5_model = AzureChatOpenAI(</span></span><br><span class=\"line\">    <span class=\"comment\">#     openai_api_version=&quot;2024-02-15-preview&quot;,</span></span><br><span class=\"line\">    <span class=\"comment\">#     azure_deployment=DEPLOYMENT_NAME_GPT3P5,</span></span><br><span class=\"line\">    <span class=\"comment\"># )</span></span><br><span class=\"line\"></span><br><span class=\"line\">    parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词</span></span><br><span class=\"line\">    system_template = <span class=\"string\">&quot;你是一个AI助手。&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 生成系统提示词 obj</span></span><br><span class=\"line\">    sys_msg = SystemMessage(content=system_template)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 对话历史，手工添加</span></span><br><span class=\"line\">    messages = []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 先添加系统消息</span></span><br><span class=\"line\">    messages.append(sys_msg)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># res = qwen_chat.invoke(&quot;hi,你好，我是卷儿&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># print(res)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 添加我们的输入</span></span><br><span class=\"line\">    messages.append(HumanMessage(content=<span class=\"string\">&quot;hi,你好，我是卷儿&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 添加Ai的回答</span></span><br><span class=\"line\">    <span class=\"comment\"># 注意： 大模型输出是一个 AIMessage， 所以我们可以这样添加</span></span><br><span class=\"line\">    <span class=\"comment\"># messages.append(res)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 我们验证一下，对话历史是否 有效果, 用模型直接根据历史回答</span></span><br><span class=\"line\">    messages.append(HumanMessage(content=<span class=\"string\">&quot;我是谁？&quot;</span>))</span><br><span class=\"line\">    <span class=\"comment\"># res = qwen_chat.invoke(messages)</span></span><br><span class=\"line\">    <span class=\"comment\"># print(res.content)</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    store = &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#Message History</span></span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_community.chat_message_histories <span class=\"keyword\">import</span> ChatMessageHistory</span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_core.chat_history <span class=\"keyword\">import</span> BaseChatMessageHistory</span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain_core.runnables.history <span class=\"keyword\">import</span> RunnableWithMessageHistory</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">get_session_history</span>(<span class=\"params\">session_id: <span class=\"built_in\">str</span></span>) -&gt; BaseChatMessageHistory:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> session_id <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> store:</span><br><span class=\"line\">            store[session_id] = ChatMessageHistory()</span><br><span class=\"line\">        <span class=\"keyword\">return</span> store[session_id]</span><br><span class=\"line\"></span><br><span class=\"line\">    model = qwen_chat</span><br><span class=\"line\">    with_message_history = RunnableWithMessageHistory(model, get_session_history)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># session_id 是一个 TAG， 一个记录，表示不同的配置，比如你有很多对话，用不同的tag 表示不同的对话历史</span></span><br><span class=\"line\">    config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;Hi! I&#x27;m Bob&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 同样的config， 同样的对话历史</span></span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 使用不同TAG，abc3， 即不同的历史</span></span><br><span class=\"line\">    config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc3&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 不认识之前提到的名字</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 用之间的TAG，就可以识别了~</span></span><br><span class=\"line\">    config = &#123;<span class=\"string\">&quot;configurable&quot;</span>: &#123;<span class=\"string\">&quot;session_id&quot;</span>: <span class=\"string\">&quot;abc2&quot;</span>&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    response = with_message_history.invoke(</span><br><span class=\"line\">        [HumanMessage(content=<span class=\"string\">&quot;What&#x27;s my name?&quot;</span>)],</span><br><span class=\"line\">        config=config,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(response.content)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> with_message_history.stream([HumanMessage(content=<span class=\"string\">&quot;hi! tell me a joke&quot;</span>)],</span><br><span class=\"line\">           config=config,</span><br><span class=\"line\">    ):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(r.content, end=<span class=\"string\">&quot;|&quot;</span>)</span><br></pre></td></tr></table></figure>","tags":["langchain"]},{"title":"Langchain系列[05]runnable系列函数","url":"/forward/a587df74.html","content":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><p>为了帮助小伙伴们更好的发挥chain 的作用以及增加chain 的灵活性，有一些函数可以参与到chain的各环节，这个文档来给大家介绍一下~</p>\n<ul>\n<li><strong>RunnableParallel</strong></li>\n<li><strong>RunnablePassthrough</strong></li>\n<li><strong>RunnableLambda</strong></li>\n<li><strong>RunnableBranch</strong></li>\n</ul>\n<hr>\n<h3 id=\"RunnableParallel\"><a href=\"#RunnableParallel\" class=\"headerlink\" title=\"RunnableParallel\"></a><strong>RunnableParallel</strong></h3><p>RunnableParallel 在将一个 Runnable 的输出调整为符合序列中下一个 Runnable 的输入格式时非常有用。在这里，提示（prompt）的输入预期是一个<strong>包含“context”和“question”键的映射（map）</strong>。用户输入仅仅是问题（question）。我们需要使用我们的检索器（retriever）获取上下文（context），并将用户输入作为“question”键下的值传递过去。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 提示词</span></span><br><span class=\"line\">template = <span class=\"string\">&quot;&quot;&quot;Answer the question based only on the following context:</span></span><br><span class=\"line\"><span class=\"string\">&#123;context&#125;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">Question: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\">prompt = ChatPromptTemplate.from_template(template)</span><br><span class=\"line\"></span><br><span class=\"line\">chain = (</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;context&quot;</span>: retriever, <span class=\"string\">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class=\"line\">        | prompt</span><br><span class=\"line\">        | model</span><br><span class=\"line\">        | output_parser</span><br><span class=\"line\">)</span><br><span class=\"line\">res = chain.invoke(<span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 问题  下面这样可以么？</span></span><br><span class=\"line\">res = chain.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">不行，相当于：<span class=\"string\">&quot;question&quot;</span>:&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这里</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 可以是这样</span></span><br><span class=\"line\">&#123;<span class=\"string\">&quot;context&quot;</span>: retriever, <span class=\"string\">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可以是这样</span></span><br><span class=\"line\">RunnableParallel(&#123;<span class=\"string\">&quot;context&quot;</span>: retriever, <span class=\"string\">&quot;question&quot;</span>: RunnablePassthrough()&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 可以是这样</span></span><br><span class=\"line\">RunnableParallel(context=retriever, question=RunnablePassthrough())</span><br></pre></td></tr></table></figure>\n\n<p><strong>重点： 还可以</strong>通过itemgetter 获得环境中的Key 值</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">chain = (</span><br><span class=\"line\">        <span class=\"comment\"># 原来是这样 &#123;&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()&#125;</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;context&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>) | retriever, &lt;----------</span><br><span class=\"line\">            <span class=\"string\">&quot;question&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>), &lt;---------</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        | prompt</span><br><span class=\"line\">        | model</span><br><span class=\"line\">        | StrOutputParser()</span><br><span class=\"line\">)</span><br><span class=\"line\">res = chain.invoke(<span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br></pre></td></tr></table></figure>\n\n<p>并发执行chain</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">joke_chain = ChatPromptTemplate.from_template(<span class=\"string\">&quot;告诉我一个关于 &#123;topic&#125;的笑话&quot;</span>) | model</span><br><span class=\"line\">topic_chain = (</span><br><span class=\"line\">        ChatPromptTemplate.from_template(<span class=\"string\">&quot;告诉我一个关于&#123;topic&#125;的话题&quot;</span>) | model</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">map_chain = RunnableParallel(joke=joke_chain, topic=topic_chain)</span><br><span class=\"line\"></span><br><span class=\"line\">res = map_chain.invoke(&#123;<span class=\"string\">&quot;topic&quot;</span>: <span class=\"string\">&quot;冰淇淋&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"RunnablePassthrough\"><a href=\"#RunnablePassthrough\" class=\"headerlink\" title=\"RunnablePassthrough\"></a><strong>RunnablePassthrough</strong></h3><p>RunnablePassthrough 允许您原封不动地传递输入，或者添加额外的键。这通常与 RunnableParallel 一起使用，以便在映射中为数据分配一个新的键。 当单独调用 RunnablePassthrough() 时，它只会简单地接收输入并直接传递出去。 当调用 RunnablePassthrough 的 assign 方法（RunnablePassthrough.assign(…)）时，它会接收输入，并将传递给 assign 函数的额外参数添加到输入中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># RunnablePassthrough</span></span><br><span class=\"line\">runnable = RunnableParallel(</span><br><span class=\"line\">    passed=RunnablePassthrough(),</span><br><span class=\"line\">    extra=RunnablePassthrough.assign(mult=<span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&quot;num&quot;</span>] * <span class=\"number\">3</span>),</span><br><span class=\"line\">    modified=<span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&quot;num&quot;</span>] + <span class=\"number\">1</span>,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">runnable.invoke(&#123;<span class=\"string\">&quot;num&quot;</span>: <span class=\"number\">1</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">------</span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;extra&#x27;</span>: &#123;<span class=\"string\">&#x27;mult&#x27;</span>: <span class=\"number\">3</span>, <span class=\"string\">&#x27;num&#x27;</span>: <span class=\"number\">1</span>&#125;, <span class=\"string\">&#x27;modified&#x27;</span>: <span class=\"number\">2</span>, <span class=\"string\">&#x27;passed&#x27;</span>: &#123;<span class=\"string\">&#x27;num&#x27;</span>: <span class=\"number\">1</span>&#125;&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"RunnableLambda\"><a href=\"#RunnableLambda\" class=\"headerlink\" title=\"RunnableLambda\"></a><strong>RunnableLambda</strong></h3><p>您可以在管道中使用任意函数。 请注意，所有输入到这些函数的参数需要是单个参数。如果您有一个接受多个参数的函数，您应该编写一个包装器，该包装器接受单个输入并将其解包为多个参数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">format_docs_into_text</span>(<span class=\"params\">docs</span>):</span><br><span class=\"line\">    doc_size = <span class=\"built_in\">len</span>(docs)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;收到 &#123;&#125; 个文档&#x27;</span>.<span class=\"built_in\">format</span>(doc_size))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> docs</span><br><span class=\"line\">chain = (</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;context&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>) | retriever | RunnableLambda(format_docs_into_text),</span><br><span class=\"line\">            <span class=\"string\">&quot;question&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>),</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        | prompt</span><br><span class=\"line\">        | model</span><br><span class=\"line\">        | StrOutputParser()</span><br><span class=\"line\">)</span><br><span class=\"line\">res = chain.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>&#125;)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br></pre></td></tr></table></figure>\n\n<p>2个函数都能正常调用，差异在哪？</p>\n<p>函数直接返回Document 对象：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">format_docs_into_text</span>(<span class=\"params\">docs</span>):</span><br><span class=\"line\">    doc_size = <span class=\"built_in\">len</span>(docs)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;收到 &#123;&#125; 个文档&#x27;</span>.<span class=\"built_in\">format</span>(doc_size))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> docs</span><br></pre></td></tr></table></figure>\n\n<p>函数直接返回字符串：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">format_docs_into_text2</span>(<span class=\"params\">docs</span>):</span><br><span class=\"line\">    doc_size = <span class=\"built_in\">len</span>(docs)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;收到 &#123;&#125; 个文档&#x27;</span>.<span class=\"built_in\">format</span>(doc_size))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot;\\n\\n&quot;</span>.join(doc.page_content <span class=\"keyword\">for</span> doc <span class=\"keyword\">in</span> docs)</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p>生成的提示词不同</p>\n<p><strong>直接返回docs 的情况</strong></p>\n<p>[HumanMessage(content=”Answer the question based only on the following context:\\n        [<strong>Document</strong>(page_content=’LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô …</p>\n<p><strong>返回字符串的情况</strong></p>\n<p>[HumanMessage(content=’Answer the question based only on the following context:\\n        <strong>LangSmith User Guide |</strong> \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\nLangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ</p>\n<hr>\n<h3 id=\"RunnableBranch\"><a href=\"#RunnableBranch\" class=\"headerlink\" title=\"RunnableBranch\"></a><strong>RunnableBranch</strong></h3><p>1个决策chain  + 3个分支 chain</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 三个分支： langchain \\ 百度 \\  缺省</span></span><br><span class=\"line\"></span><br><span class=\"line\">    langchain_chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;你是langchain 专家，回答下面的问题:</span></span><br><span class=\"line\"><span class=\"string\">                问题: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">                回答:&quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">    )</span><br><span class=\"line\">    baidu_chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;你是百度AI专家，回答下面的问题:</span></span><br><span class=\"line\"><span class=\"string\">                问题: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">                回答:&quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">    )</span><br><span class=\"line\">    general_chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">            <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">            回答下面的问题：&#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">            &quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 选择分支</span></span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;基于用户问题，选择这个问题是属于 `LangChain`, `百度`, or `其他`.</span></span><br><span class=\"line\"><span class=\"string\">        不要返回多余的词。</span></span><br><span class=\"line\"><span class=\"string\">        &lt;question&gt;</span></span><br><span class=\"line\"><span class=\"string\">        &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">        &lt;/question&gt;</span></span><br><span class=\"line\"><span class=\"string\">        分类:&quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">            | StrOutputParser()</span><br><span class=\"line\">    )</span><br></pre></td></tr></table></figure>\n\n<p>选择分支</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">route</span>(<span class=\"params\">info</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">&quot;百度&quot;</span> <span class=\"keyword\">in</span> info[<span class=\"string\">&quot;topic&quot;</span>].lower():</span><br><span class=\"line\">            <span class=\"keyword\">return</span> baidu_chain</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> <span class=\"string\">&quot;langchain&quot;</span> <span class=\"keyword\">in</span> info[<span class=\"string\">&quot;topic&quot;</span>].lower():</span><br><span class=\"line\">            <span class=\"keyword\">return</span> langchain_chain</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> general_chain</span><br><span class=\"line\"><span class=\"comment\"># ---------------</span></span><br><span class=\"line\"><span class=\"comment\"># 结果</span></span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;question&#x27;</span>: <span class=\"string\">&#x27;我如何使用百度？&#x27;</span>, <span class=\"string\">&#x27;topic&#x27;</span>: <span class=\"string\">&#x27;百度&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>\n\n<p>整个链路</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">full_chain = &#123;<span class=\"string\">&quot;topic&quot;</span>: chain, <span class=\"string\">&quot;question&quot;</span>: <span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&quot;question&quot;</span>]&#125; | RunnableLambda(</span><br><span class=\"line\">    route</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">res = full_chain.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;我如何使用百度？&quot;</span>&#125;)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br></pre></td></tr></table></figure>\n\n<p>完整代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> uuid</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models.baidu_qianfan_endpoint <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.embeddings <span class=\"keyword\">import</span> QianfanEmbeddingsEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.vectorstores.chroma <span class=\"keyword\">import</span> Chroma</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.messages <span class=\"keyword\">import</span> HumanMessage</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompt_values <span class=\"keyword\">import</span> ChatPromptValue</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate, PromptTemplate</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables <span class=\"keyword\">import</span> RunnablePassthrough, RunnableParallel, RunnableLambda</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> operator <span class=\"keyword\">import</span> itemgetter</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_ACCESS_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_ACCESS_KEY&#x27;</span>)</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_SECRET_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_SECRET_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    unique_id = uuid.uuid4().<span class=\"built_in\">hex</span>[<span class=\"number\">0</span>:<span class=\"number\">8</span>]</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;LANGCHAIN_PROJECT&quot;</span>] = <span class=\"string\">f&quot; [返回docs]轨迹 - <span class=\"subst\">&#123;unique_id&#125;</span>&quot;</span></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;LANGCHAIN_TRACING_V2&quot;</span>] = <span class=\"string\">&#x27;true&#x27;</span></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;LANGCHAIN_API_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_LANGCHAIN_API_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = QianfanChatEndpoint(</span><br><span class=\"line\">        model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 检索chain</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 千帆嵌入模型</span></span><br><span class=\"line\">    embeddings_model = QianfanEmbeddingsEndpoint(model=<span class=\"string\">&quot;bge_large_en&quot;</span>, endpoint=<span class=\"string\">&quot;bge_large_en&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 载入数据库</span></span><br><span class=\"line\">    vector_store = Chroma(persist_directory=<span class=\"string\">&quot;D:\\\\LLM\\\\my_projects\\\\chroma_db&quot;</span>, embedding_function=embeddings_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 创建检索器</span></span><br><span class=\"line\">    retriever = vector_store.as_retriever()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词</span></span><br><span class=\"line\">    template = <span class=\"string\">&quot;&quot;&quot;Answer the question based only on the following context:</span></span><br><span class=\"line\"><span class=\"string\">        &#123;context&#125;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Question: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">    prompt = ChatPromptTemplate.from_template(template)</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            <span class=\"comment\"># 原来是这样 &#123;&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()&#125;</span></span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;context&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>) | retriever,</span><br><span class=\"line\">                <span class=\"string\">&quot;question&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>),</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">            | model</span><br><span class=\"line\">            | StrOutputParser()</span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"comment\"># res = chain.invoke(&quot;how can langsmith help with testing?&quot;)</span></span><br><span class=\"line\">    <span class=\"comment\"># print(res)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 并行执行</span></span><br><span class=\"line\">    joke_chain = ChatPromptTemplate.from_template(<span class=\"string\">&quot;告诉我一个关于 &#123;topic&#125;的笑话&quot;</span>) | model</span><br><span class=\"line\">    topic_chain = (</span><br><span class=\"line\">            ChatPromptTemplate.from_template(<span class=\"string\">&quot;告诉我一个关于&#123;topic&#125;的话题&quot;</span>) | model</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    map_chain = RunnableParallel(joke=joke_chain, topic=topic_chain)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># res = map_chain.invoke(&#123;&quot;topic&quot;: &quot;冰淇淋&quot;&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># RunnablePassthrough</span></span><br><span class=\"line\">    runnable = RunnableParallel(</span><br><span class=\"line\">        passed=RunnablePassthrough(),</span><br><span class=\"line\">        extra=RunnablePassthrough.assign(mult=<span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&quot;num&quot;</span>] * <span class=\"number\">3</span>),</span><br><span class=\"line\">        modified=<span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&quot;num&quot;</span>] + <span class=\"number\">1</span>,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># res = runnable.invoke(&#123;&quot;num&quot;: 1&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#RunnableLambda</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">format_docs_into_text</span>(<span class=\"params\">docs</span>):</span><br><span class=\"line\">        doc_size = <span class=\"built_in\">len</span>(docs)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;收到 &#123;&#125; 个文档&#x27;</span>.<span class=\"built_in\">format</span>(doc_size))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> docs</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">format_docs_into_text2</span>(<span class=\"params\">docs</span>):</span><br><span class=\"line\">        doc_size = <span class=\"built_in\">len</span>(docs)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;收到 &#123;&#125; 个文档&#x27;</span>.<span class=\"built_in\">format</span>(doc_size))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"string\">&quot;\\n\\n&quot;</span>.join(doc.page_content <span class=\"keyword\">for</span> doc <span class=\"keyword\">in</span> docs)</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;context&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>) | retriever | RunnableLambda(format_docs_into_text),</span><br><span class=\"line\">                <span class=\"string\">&quot;question&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>),</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">            | model</span><br><span class=\"line\">            | StrOutputParser()</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    chain_test = (</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;context&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>) | retriever | RunnableLambda(format_docs_into_text),</span><br><span class=\"line\">                <span class=\"string\">&quot;question&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>),</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">    )</span><br><span class=\"line\">    res = chain_test.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    chain_test_2 = (</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;context&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>) | retriever | RunnableLambda(format_docs_into_text2),</span><br><span class=\"line\">                <span class=\"string\">&quot;question&quot;</span>: itemgetter(<span class=\"string\">&quot;question&quot;</span>),</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">    )</span><br><span class=\"line\">    res = chain_test_2.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    res = chain.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>&#125;)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># RunnableBranch</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 三个分支： langchain \\ 百度 \\  缺省</span></span><br><span class=\"line\"></span><br><span class=\"line\">    langchain_chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;你是langchain 专家，回答下面的问题:</span></span><br><span class=\"line\"><span class=\"string\">                问题: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">                回答:&quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">    )</span><br><span class=\"line\">    baidu_chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;你是百度AI专家，回答下面的问题:</span></span><br><span class=\"line\"><span class=\"string\">                问题: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">                回答:&quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">    )</span><br><span class=\"line\">    general_chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">            <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">            回答下面的问题：&#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">            &quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 选择分支</span></span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            PromptTemplate.from_template(</span><br><span class=\"line\">                <span class=\"string\">&quot;&quot;&quot;基于用户问题，选择这个问题是属于 `LangChain`, `百度`, or `其他`.</span></span><br><span class=\"line\"><span class=\"string\">        不要返回多余的词。</span></span><br><span class=\"line\"><span class=\"string\">        &lt;question&gt;</span></span><br><span class=\"line\"><span class=\"string\">        &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">        &lt;/question&gt;</span></span><br><span class=\"line\"><span class=\"string\">        分类:&quot;&quot;&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            | model</span><br><span class=\"line\">            | StrOutputParser()</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 情况判断</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">route</span>(<span class=\"params\">info</span>):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"string\">&quot;百度&quot;</span> <span class=\"keyword\">in</span> info[<span class=\"string\">&quot;topic&quot;</span>].lower():</span><br><span class=\"line\">            <span class=\"keyword\">return</span> baidu_chain</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> <span class=\"string\">&quot;langchain&quot;</span> <span class=\"keyword\">in</span> info[<span class=\"string\">&quot;topic&quot;</span>].lower():</span><br><span class=\"line\">            <span class=\"keyword\">return</span> langchain_chain</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> general_chain</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    full_chain = &#123;<span class=\"string\">&quot;topic&quot;</span>: chain, <span class=\"string\">&quot;question&quot;</span>: <span class=\"keyword\">lambda</span> x: x[<span class=\"string\">&quot;question&quot;</span>]&#125; | RunnableLambda(</span><br><span class=\"line\">        route</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    res = full_chain.invoke(&#123;<span class=\"string\">&quot;question&quot;</span>: <span class=\"string\">&quot;我如何使用百度？&quot;</span>&#125;)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res)</span><br></pre></td></tr></table></figure>","tags":["langchain"]},{"title":"Langchain系列[04]表达式语言LCEL","url":"/forward/1f042660.html","content":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">- 了解LCEL包含的组件，可以调用的方法。</span><br><span class=\"line\"></span><br><span class=\"line\">- 链式调用基本形式</span><br></pre></td></tr></table></figure>\n\n<p><strong>LangChain 表达式语言（LCEL）是一种让用户能够更容易地组合不同链条的声明式编程方式。</strong></p>\n<p><strong>之前我们介绍了chain 中的组件，这个文档来明确一下组件之间的输入和输出</strong></p>\n<p>LCEL 从一开始就是为了支持将原型直接投入生产而设计的，无需修改任何代码。这适用于从最简单的“提示+大语言模型”链条到最复杂的链条（我们已经看到有人成功在生产环境中运行包含数百个步骤的 LCEL 链条）。以下是您可能想使用 LCEL 的几个原因：</p>\n<ol>\n<li><p><strong>流支持</strong>：使用 LCEL 构建链条时，<strong>您可以从第一个令牌开始就获得最快的响应时间</strong>（即从输出开始到第一个数据块出现的时间）。对于某些链条来说，这意味着我们可以直接从大语言模型向输出解析器发送令牌流，您将以与模型提供者输出原始令牌相同的速度接收到解析后的数据块。</p>\n</li>\n<li><p><strong>异步支持</strong>：使用 LCEL 构建的任何链条都可以通过同步 API（例如，在 Jupyter  笔记本中测试原型）或异步 API（例如，在 LangServe  服务器中）调用。这使得您可以使用相同的代码进行原型设计和生产，同时还能保持出色的性能，并能在同一服务器上处理多个并发请求。</p>\n<blockquote>\n<p>同步 API：当您在 Jupyter 笔记本或其他环境中直接调用链条时，您会使用同步 API。这种方式会阻塞当前进程，直到链条执行完毕并返回结果。这适合于原型设计和开发阶段，因为它允许您立即看到每个步骤的结果。</p>\n</blockquote>\n<blockquote>\n<p>异步 API：在生产环境中，您通常希望处理多个请求而不会阻塞主进程。这时，您可以使用异步  API。当您通过异步方式调用链条时，请求会立即返回一个唯一标识符或“任务ID”，而不是阻塞并等待结果。您可以在稍后的时间点，使用这个任务ID来检查链条的状态或获取结果。这种方式允许服务器同时处理多个请求，从而提高了效率和性能。</p>\n</blockquote>\n</li>\n<li><p><strong>并行执行优化</strong>：当 LCEL 链条中有可以并行执行的步骤（例如，从多个检索器获取文档）时，系统会自动进行优化，无论是在同步还是异步接口中，都能实现最小的延迟。</p>\n</li>\n<li><p>重试和回退：您可以为 LCEL 链条的任何部分配置重试和回退策略。这是在大规模应用中提高链条可靠性的好方法。我们目前正在为重试和回退添加流支持，这样您就可以在不增加延迟的情况下获得更高的可靠性。</p>\n</li>\n<li><p><strong>访问中间结果</strong>：对于复杂的链条，通常在最终输出完成之前访问中间步骤的结果是非常有用的。这可以用来通知最终用户正在发生的事情，或者只是用来调试您的链条。您可以流式传输中间结果，并且可以在每个 LangServe 服务器上使用。</p>\n</li>\n<li><p><strong>输入和输出架构</strong>：LCEL 链条都有 Pydantic 和 JSONSchema 架构，这些架构是从链条的结构中自动推断出来的。这可以用来验证输入和输出，并且是 LangServe 不可或缺的一部分。</p>\n</li>\n<li><p>与 LangSmith 的无缝集成：随着链条变得越来越复杂，了解每个步骤的具体情况变得越来越重要。使用 LCEL，所有步骤都会自动记录到 LangSmith 中，以便进行最大程度的监控和调试。</p>\n</li>\n<li><p>与 LangServe 的无缝部署集成：使用 LCEL 创建的任何链条都可以轻松地通过 LangServe 部署。</p>\n</li>\n</ol>\n<hr>\n<h1 id=\"快速开始\"><a href=\"#快速开始\" class=\"headerlink\" title=\"快速开始\"></a>快速开始</h1><p>基础链: prompt + model + output parser</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 环境设置</span></span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;QIANFAN_ACCESS_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_ACCESS_KEY&#x27;</span>)</span><br><span class=\"line\">os.environ[<span class=\"string\">&quot;QIANFAN_SECRET_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_SECRET_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># chatModel</span></span><br><span class=\"line\">model = QianfanChatEndpoint(</span><br><span class=\"line\">    model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span></span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 提示词</span></span><br><span class=\"line\">prompt = ChatPromptTemplate.from_template(<span class=\"string\">&quot;给我讲一个关于&#123;topic&#125;的笑话&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出解析器</span></span><br><span class=\"line\">output_parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 基本链</span></span><br><span class=\"line\">chain = prompt | model | output_parser</span><br><span class=\"line\"></span><br><span class=\"line\">res = chain.invoke(&#123;<span class=\"string\">&quot;topic&quot;</span>: <span class=\"string\">&quot;冰淇淋&quot;</span>&#125;)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\">----</span><br><span class=\"line\"><span class=\"comment\"># 输出</span></span><br><span class=\"line\">当然可以，这是一个关于冰淇淋的笑话：...</span><br><span class=\"line\">----</span><br></pre></td></tr></table></figure>\n\n<p>这里的 “|” 符号有点像 Unix  系统中的管道操作符，它把不同的部分连接起来，把一个部分的输出作为下一个部分的输入。在这个流程中，用户输入首先传给提示模板，然后提示模板的输出传给模型，模型的输出再传给输出解析器。我们来逐一看看每个部分，以便更好地理解整个流程。</p>\n<hr>\n<ol>\n<li>提示词（Prompt） 提示是一个基础的提示模板（BasePromptTemplate），这意味着它接收一个模板变量的字典，并生成一个提示值（PromptValue）。提示值是对已完成提示的包装，可以传递给大语言模型（LLM，它接受字符串作为输入）或聊天模型（ChatModel，它接受一系列的消息作为输入）。它能够与任何语言模型类型配合工作，因为它定义了生成基础消息（BaseMessages）和生成字符串的逻辑。</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 提示词</span></span><br><span class=\"line\">prompt_value = prompt.invoke(&#123;<span class=\"string\">&quot;topic&quot;</span>: <span class=\"string\">&quot;冰淇淋&quot;</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">cp_value = ChatPromptValue(messages=[HumanMessage(content=<span class=\"string\">&#x27;给我讲一个关于冰淇淋的笑话&#x27;</span>)])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># prompt_value 和 cp_value 相同</span></span><br><span class=\"line\"></span><br><span class=\"line\">message = prompt_value.to_messages()</span><br><span class=\"line\"><span class=\"comment\"># 输出 [content=&#x27;给我讲一个关于冰淇淋的笑话&#x27;]</span></span><br><span class=\"line\"></span><br><span class=\"line\">pr0_str = prompt_value.to_string()</span><br><span class=\"line\"><span class=\"comment\"># 输出  &#x27;Human: 给我讲一个关于冰淇淋的笑话&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<ol>\n<li>模型（Model） 提示词接着被传递给模型。在这个例子中，我们的模型是一个聊天模型（ChatModel），这意味着它将输出一个基础消息（BaseMessage）。</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">model = QianfanChatEndpoint(</span><br><span class=\"line\">    model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span></span><br><span class=\"line\">)</span><br><span class=\"line\">...</span><br><span class=\"line\">message = model.invoke(prompt_value)</span><br><span class=\"line\"><span class=\"comment\"># message 是一个 AIMessage类的对象</span></span><br></pre></td></tr></table></figure>\n\n<ol>\n<li>输出解析器（Output parser） 最后，我们将模型输出传递给输出解析器，它是一个基础的输出解析器（BaseOutputParser），这意味着它接受字符串或基础消息（BaseMessage）作为输入。StrOutputParser 特别是将任何输入简单转换为字符串。</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 解析器</span></span><br><span class=\"line\">output_parser = StrOutputParser()</span><br><span class=\"line\">res = output_parser.invoke(message)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出 ：当然可以，这是一个关于冰淇淋的笑话...</span></span><br></pre></td></tr></table></figure>\n\n<ol>\n<li>整个流程（Entire Pipeline） 让我们一步一步地跟随这个过程：</li>\n</ol>\n<ul>\n<li>首先，我们输入用户对所需主题的输入，例如 **{“topic”: “ice cream”}**。</li>\n<li>提示组件接收用户输入，并使用该主题来构建提示，然后生成一个 <strong>PromptValue</strong>。</li>\n<li>模型组件接收生成的提示，并将其传递给 OpenAI 大语言模型进行评估。模型生成的输出是一个 <strong>ChatMessage</strong> 对象。</li>\n<li>最后，输出解析器组件接收一个 ChatMessage，并将其转换成一个 Python <strong>字符串</strong>，这个字符串从 invoke 方法中返回。</li>\n</ul>\n<p>可以这么调用么？</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># res = chain.invoke(&#123;&quot;topic&quot;: &quot;冰淇淋&quot;&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 注意，由于需要指定参数，下面这样调用是不行的</span></span><br><span class=\"line\">res = chain.invoke(<span class=\"string\">&quot;冰淇淋&quot;</span>)  &lt;--------- 不行</span><br></pre></td></tr></table></figure>\n\n<p>需要指定参数传给谁 （RunnablePassthrough() 从输入接收参数，传递给topic）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">chain = (</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;topic&quot;</span>: RunnablePassthrough()&#125;</span><br><span class=\"line\">        | prompt</span><br><span class=\"line\">        | model</span><br><span class=\"line\">        | output_parser</span><br><span class=\"line\">)</span><br><span class=\"line\">res = chain.invoke(<span class=\"string\">&quot;冰淇淋&quot;</span>) &lt;----------可以</span><br></pre></td></tr></table></figure>\n\n<p><strong>检索流程 （了解一下，后面详解）</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 千帆嵌入模型</span></span><br><span class=\"line\">embeddings_model = QianfanEmbeddingsEndpoint(model=<span class=\"string\">&quot;bge_large_en&quot;</span>, endpoint=<span class=\"string\">&quot;bge_large_en&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 载入数据库</span></span><br><span class=\"line\">vector_store = Chroma(persist_directory=<span class=\"string\">&quot;D:\\\\LLM\\\\my_projects\\\\chroma_db&quot;</span>, embedding_function=embeddings_model)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建检索器</span></span><br><span class=\"line\">retriever = vector_store.as_retriever()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 提示词</span></span><br><span class=\"line\">template = <span class=\"string\">&quot;&quot;&quot;Answer the question based only on the following context:</span></span><br><span class=\"line\"><span class=\"string\">&#123;context&#125;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">Question: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\">prompt = ChatPromptTemplate.from_template(template)</span><br><span class=\"line\"></span><br><span class=\"line\">chain = (</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;context&quot;</span>: retriever, <span class=\"string\">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class=\"line\">        | prompt</span><br><span class=\"line\">        | model</span><br><span class=\"line\">        | output_parser</span><br><span class=\"line\">)</span><br><span class=\"line\">res = chain.invoke(<span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(res)</span><br></pre></td></tr></table></figure>\n\n<p>完整代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models.baidu_qianfan_endpoint <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.embeddings <span class=\"keyword\">import</span> QianfanEmbeddingsEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.vectorstores.chroma <span class=\"keyword\">import</span> Chroma</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.messages <span class=\"keyword\">import</span> HumanMessage</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompt_values <span class=\"keyword\">import</span> ChatPromptValue</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables <span class=\"keyword\">import</span> RunnablePassthrough</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_ACCESS_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_ACCESS_KEY&#x27;</span>)</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_SECRET_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_SECRET_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    model = QianfanChatEndpoint(</span><br><span class=\"line\">        model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    prompt = ChatPromptTemplate.from_template(<span class=\"string\">&quot;给我讲一个关于&#123;topic&#125;的笑话&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    output_parser = StrOutputParser()</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = prompt | model | output_parser</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># res = chain.invoke(&#123;&quot;topic&quot;: &quot;冰淇淋&quot;&#125;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 注意，由于需要指定参数，下面这样调用是不行的</span></span><br><span class=\"line\">    <span class=\"comment\"># res = chain.invoke(&quot;冰淇淋&quot;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            &#123;<span class=\"string\">&quot;topic&quot;</span>: RunnablePassthrough()&#125;</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">            | model</span><br><span class=\"line\">            | output_parser</span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"comment\"># res = chain.invoke(&quot;冰淇淋&quot;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># print(res)</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词</span></span><br><span class=\"line\">    prompt_value = prompt.invoke(&#123;<span class=\"string\">&quot;topic&quot;</span>: <span class=\"string\">&quot;冰淇淋&quot;</span>&#125;)</span><br><span class=\"line\">    prompt_value</span><br><span class=\"line\"></span><br><span class=\"line\">    cp_value = ChatPromptValue(messages=[HumanMessage(content=<span class=\"string\">&#x27;给我讲一个关于冰淇淋的笑话&#x27;</span>)])</span><br><span class=\"line\"></span><br><span class=\"line\">    message = prompt_value.to_messages()</span><br><span class=\"line\"></span><br><span class=\"line\">    pr0_str = prompt_value.to_string()</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 模型</span></span><br><span class=\"line\">    message = model.invoke(prompt_value)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(message)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 解析器</span></span><br><span class=\"line\">    output_parser = StrOutputParser()</span><br><span class=\"line\">    res = output_parser.invoke(message)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 检索chain</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 千帆嵌入模型</span></span><br><span class=\"line\">    embeddings_model = QianfanEmbeddingsEndpoint(model=<span class=\"string\">&quot;bge_large_en&quot;</span>, endpoint=<span class=\"string\">&quot;bge_large_en&quot;</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 载入本地向量数据库 Chr</span></span><br><span class=\"line\">    vector_store = Chroma(persist_directory=<span class=\"string\">&quot;D:\\\\LLM\\\\my_projects\\\\chroma_db&quot;</span>, embedding_function=embeddings_model)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 创建检索器</span></span><br><span class=\"line\">    retriever = vector_store.as_retriever()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词</span></span><br><span class=\"line\">    template = <span class=\"string\">&quot;&quot;&quot;Answer the question based only on the following context:</span></span><br><span class=\"line\"><span class=\"string\">    &#123;context&#125;</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Question: &#123;question&#125;</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    prompt = ChatPromptTemplate.from_template(template)</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            &#123;<span class=\"string\">&quot;context&quot;</span>: retriever, <span class=\"string\">&quot;question&quot;</span>: RunnablePassthrough()&#125;</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">            | model</span><br><span class=\"line\">            | output_parser</span><br><span class=\"line\">    )</span><br><span class=\"line\">    res = chain.invoke(<span class=\"string\">&quot;how can langsmith help with testing?&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(res)</span><br></pre></td></tr></table></figure>","tags":["langchain"]},{"title":"Langchain系列[06]语言模型","url":"/forward/fa3d6d2f.html","content":"<h2 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h2><p><strong>模型入门</strong></p>\n<p>Langchain 2.0 都找不到下面这个图了，藏到了1.0 里面…</p>\n<p><strong>语言模型</strong>是核心，我们需要熟悉跟语言模型相关的消息：系统消息、AI消息、Human消息、工具消息…</p>\n<p><img src=\"/images/Langchain%E7%B3%BB%E5%88%97-01-%E4%BB%8B%E7%BB%8D.assets/image-20240627100437691.png\" alt=\"image-20240627100437691\"></p>\n<p><strong>llm</strong> vs  <strong>chatModel</strong></p>\n<p><strong><code>百度千帆</code></strong></p>\n<p><strong>对话</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">chat_comp = qianfan.ChatCompletion(access_key=<span class=\"string\">&quot;...&quot;</span>, secret_key=<span class=\"string\">&quot;...&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调用默认模型，即 ERNIE-Bot-turbo</span></span><br><span class=\"line\">resp = chat_comp.do(messages=[&#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;user&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;你好&quot;</span></span><br><span class=\"line\">&#125;])</span><br></pre></td></tr></table></figure>\n\n<p><strong>完成</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">comp = qianfan.Completion()</span><br><span class=\"line\"></span><br><span class=\"line\">resp = comp.do(model=<span class=\"string\">&quot;ERNIE-Bot&quot;</span>, prompt=<span class=\"string\">&quot;你好&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 输出：你好！有什么我可以帮助你的吗？</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 续写功能同样支持流式调用</span></span><br><span class=\"line\">resp = comp.do(model=<span class=\"string\">&quot;ERNIE-Bot&quot;</span>, prompt=<span class=\"string\">&quot;你好&quot;</span>, stream=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> r <span class=\"keyword\">in</span> resp:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(r[<span class=\"string\">&#x27;result&#x27;</span>])</span><br></pre></td></tr></table></figure>\n\n<p><strong><code>Azure Openai</code></strong></p>\n<p><strong>完成</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">start_phrase = <span class=\"string\">&#x27;Write a tagline for an ice cream shop. &#x27;</span></span><br><span class=\"line\">response = openai.Completion.create(engine=deployment_name, prompt=start_phrase, max_tokens=<span class=\"number\">10</span>)</span><br><span class=\"line\">text = response[<span class=\"string\">&#x27;choices&#x27;</span>][<span class=\"number\">0</span>][<span class=\"string\">&#x27;text&#x27;</span>].replace(<span class=\"string\">&#x27;\\n&#x27;</span>, <span class=\"string\">&#x27;&#x27;</span>).replace(<span class=\"string\">&#x27; .&#x27;</span>, <span class=\"string\">&#x27;.&#x27;</span>).strip()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(start_phrase+text)</span><br></pre></td></tr></table></figure>\n\n<p><strong>对话</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">response = openai.ChatCompletion.create(</span><br><span class=\"line\">    engine=<span class=\"string\">&quot;gpt-35-turbo&quot;</span>, <span class=\"comment\"># engine = &quot;deployment_name&quot;.</span></span><br><span class=\"line\">    messages=[</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;system&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;Does Azure OpenAI support customer managed keys?&quot;</span>&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;assistant&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;Yes, customer managed keys are supported by Azure OpenAI.&quot;</span>&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;Do other Azure AI services support this too?&quot;</span>&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p><strong>消息</strong> <strong>ChatModels</strong>  接收一个消息列表作为输入，并返回一个消息。消息有几种不同的类型。所有消息都有一个角色（role）和一个内容（content）属性。角色描述了是谁在说话。LangChain 为不同的角色提供了不同的消息类。内容属性描述了消息的内容。这可以是以下几种不同的东西：</p>\n<ul>\n<li>一个字符串（大多数模型都是这种方式）</li>\n<li>一个字典列表（这用于多模态输入，其中字典包含有关该输入类型和位置的信息） 此外，消息还有一个 additional_kwargs 属性。这是传递关于消息的额外信息的地方。这主要用于特定于提供商的输入参数，而不是通用的。最著名的例子是 OpenAI 的 function_call。</li>\n</ul>\n<p><code>人类消息</code>（HumanMessage） 这代表用户的消息。通常只包含内容。</p>\n<p><code>AI消息</code>（AIMessage） 这代表模型的消息。这可能有 additional_kwargs ——例如，如果使用 OpenAI 函数调用，则可能有 functional_call。</p>\n<p><code>系统消息</code>（SystemMessage） 这代表系统消息。只有一些模型支持这个。这告诉模型如何表现。这通常只包含内容。</p>\n<p><code>函数消息</code>（FunctionMessage） 这代表函数调用的结果。除了角色和内容之外，这个消息还有一个 name 参数，它传达了产生这个结果的函数的名称。</p>\n<p><code>工具消息</code>（ToolMessage） 这代表工具调用的结果。这与 FunctionMessage 是不同的，以匹配 OpenAI 的函数和工具消息类型。除了角色和内容之外，这个消息还有一个 tool_call_id 参数，它传达了产生这个结果的工具调用的 id。</p>\n<hr>\n<p>函数调用 function call</p>\n<p>定义函数： 将2个数相加</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">function = &#123;</span><br><span class=\"line\">    <span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;add_two_int&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;将输入的2个整数相加&quot;</span>,</span><br><span class=\"line\">    <span class=\"string\">&quot;parameters&quot;</span>: &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;object&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;properties&quot;</span>: &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;int_a&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;一个整数&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;string&quot;</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            <span class=\"string\">&quot;int_b&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;一个整数&quot;</span>,</span><br><span class=\"line\">                <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;string&quot;</span></span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        <span class=\"string\">&quot;required&quot;</span>: [<span class=\"string\">&quot;int_a&quot;</span>, <span class=\"string\">&quot;int_b&quot;</span>]</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>绑定模型</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">chat_with_tools = azure_chat.bind(</span><br><span class=\"line\">  function_call=&#123;<span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;add_two_int&quot;</span>&#125;, functions=[function]</span><br><span class=\"line\">)</span><br><span class=\"line\">res = chat_with_tools.invoke(<span class=\"string\">&quot;3加18等于多少&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>结果</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">&#123;<span class=\"string\">&#x27;function_call&#x27;</span>: </span><br><span class=\"line\"> &#123;<span class=\"string\">&#x27;arguments&#x27;</span>: <span class=\"string\">&#x27;&#123;\\n  &quot;int_a&quot;: &quot;3&quot;,\\n  &quot;int_b&quot;: &quot;18&quot;\\n&#125;&#x27;</span>, <span class=\"string\">&#x27;name&#x27;</span>: <span class=\"string\">&#x27;add_two_int&#x27;</span>&#125;&#125;</span><br></pre></td></tr></table></figure>\n\n<p>绑定运行时参数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 运行时配置参数</span></span><br><span class=\"line\">res = azure_chat.with_config(configurable=&#123;<span class=\"string\">&quot;llm_temperature&quot;</span>: <span class=\"number\">0.9</span>&#125;).invoke(<span class=\"string\">&quot;给出一个随机数&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置多个模型，运行时根据需求调用</span></span><br><span class=\"line\">llm = QianfanChatEndpoint(temperature=<span class=\"number\">0.7</span>).configurable_alternatives(</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 设置可以配置的项目</span></span><br><span class=\"line\">    ConfigurableField(<span class=\"built_in\">id</span>=<span class=\"string\">&quot;llm&quot;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 设置缺省值，默认用百度</span></span><br><span class=\"line\">    default_key=<span class=\"string\">&quot;baidu4&quot;</span>,</span><br><span class=\"line\">    <span class=\"comment\"># 可以选择 Azure OpenAI</span></span><br><span class=\"line\">    azure_openai=azure_chat,</span><br><span class=\"line\">)</span><br><span class=\"line\">chain = prompt | llm</span><br><span class=\"line\"><span class=\"comment\"># 调用百度</span></span><br><span class=\"line\">res_ = chain.with_config(configurable=&#123;<span class=\"string\">&quot;llm&quot;</span>: <span class=\"string\">&quot;baidu4&quot;</span>&#125;).invoke(&#123;<span class=\"string\">&quot;product&quot;</span>: <span class=\"string\">&quot;铅笔&quot;</span>&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调用 Azure Opena</span></span><br><span class=\"line\">res_ = chain.with_config(configurable=&#123;<span class=\"string\">&quot;llm&quot;</span>: <span class=\"string\">&quot;azure_openai&quot;</span>&#125;).invoke(&#123;<span class=\"string\">&quot;product&quot;</span>: <span class=\"string\">&quot;铅笔&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>\n\n<hr>\n<p><strong>聊天模型接受消息作为输入，并返回一个消息作为输出。</strong> LangChain 有一些内置的消息类型： 系统消息（SystemMessage）：用于引导 AI 行为，通常作为输入消息序列中的第一个消息传递。 人类消息（HumanMessage）：代表与聊天模型互动的人的消息。 AI消息（AIMessage）：代表聊天模型的消息。这可以是文本或调用工具的请求。 函数消息/工具消息（FunctionMessage / ToolMessage）：用于将工具调用的结果传递回模型的消息。</p>\n<p><strong>让chat 有记忆：维护一个消息列表</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">response = openai.ChatCompletion.create(</span><br><span class=\"line\">    engine=<span class=\"string\">&quot;gpt-35-turbo&quot;</span>, <span class=\"comment\"># engine = &quot;deployment_name&quot;.</span></span><br><span class=\"line\">    messages=[</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;system&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;Does Azure OpenAI support customer managed keys?&quot;</span>&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;assistant&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;Yes, customer managed keys are supported by Azure OpenAI.&quot;</span>&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;Do other Azure AI services support this too?&quot;</span>&#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<p>在提示词中保存对话内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">prompt = ChatPromptTemplate.from_messages(</span><br><span class=\"line\">    [</span><br><span class=\"line\">        (<span class=\"string\">&quot;system&quot;</span>, <span class=\"string\">&quot;你是AI助手&quot;</span>),</span><br><span class=\"line\">        MessagesPlaceholder(variable_name=<span class=\"string\">&quot;history&quot;</span>), &lt;---虚位以待</span><br><span class=\"line\">        (<span class=\"string\">&quot;human&quot;</span>, <span class=\"string\">&quot;&#123;input&#125;&quot;</span>),</span><br><span class=\"line\">    ]</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">memory = ConversationBufferMemory(return_messages=<span class=\"literal\">True</span>) &lt;-- 想象成一个<span class=\"built_in\">list</span></span><br><span class=\"line\">res = memory.load_memory_variables(&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">   ------</span><br><span class=\"line\">    <span class=\"comment\"># 内容 &#123;&#x27;history&#x27;: []&#125;</span></span><br></pre></td></tr></table></figure>\n\n<p>保存对话记录</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">inputs = &#123;<span class=\"string\">&quot;input&quot;</span>: <span class=\"string\">&quot;你好！我是茉卷&quot;</span>&#125;</span><br><span class=\"line\">response = chain.invoke(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\">memory.save_context(inputs, &#123;<span class=\"string\">&quot;output&quot;</span>: response.content&#125;)</span><br><span class=\"line\">res = memory.load_memory_variables(&#123;&#125;)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">    inputs = &#123;<span class=\"string\">&quot;input&quot;</span>: <span class=\"string\">&quot;我叫什么名字&quot;</span>&#125;</span><br><span class=\"line\">    response = chain.invoke(inputs)</span><br><span class=\"line\">content=<span class=\"string\">&#x27;你叫茉卷。如果你有其他疑问或者需要帮助，请随时告诉我，我会尽力为你服务。&#x27;</span> additional_kwargs=&#123;<span class=\"string\">&#x27;finish_reason&#x27;</span>: <span class=\"string\">&#x27;normal&#x27;</span>, <span class=\"string\">&#x27;request_id&#x27;</span>: <span class=\"string\">&#x27;as-jjdgubp1k9&#x27;</span>, <span class=\"string\">&#x27;object&#x27;</span>: <span class=\"string\">&#x27;chat.completion&#x27;</span>, <span class=\"string\">&#x27;search_info&#x27;</span>: []&#125; response_metadata=&#123;<span class=\"string\">&#x27;finish_reason&#x27;</span>: <span class=\"string\">&#x27;normal&#x27;</span>, <span class=\"string\">&#x27;id&#x27;</span>: <span class=\"string\">&#x27;as-jjdgubp1k9&#x27;</span>, <span class=\"string\">&#x27;object&#x27;</span>: <span class=\"string\">&#x27;chat.completion&#x27;</span>, <span class=\"string\">&#x27;created&#x27;</span>: <span class=\"number\">1711101413</span>, <span class=\"string\">&#x27;result&#x27;</span>: <span class=\"string\">&#x27;你叫茉卷。如果你有其他疑问或者需要帮助，请随时告诉我，我会尽力为你服务。&#x27;</span>, <span class=\"string\">&#x27;is_truncated&#x27;</span>: <span class=\"literal\">False</span>, <span class=\"string\">&#x27;need_clear_history&#x27;</span>: <span class=\"literal\">False</span>, <span class=\"string\">&#x27;usage&#x27;</span>: &#123;<span class=\"string\">&#x27;prompt_tokens&#x27;</span>: <span class=\"number\">37</span>, <span class=\"string\">&#x27;completion_tokens&#x27;</span>: <span class=\"number\">19</span>, <span class=\"string\">&#x27;total_tokens&#x27;</span>: <span class=\"number\">56</span>&#125;&#125;</span><br></pre></td></tr></table></figure>\n\n<p>本质是一个list []， 不断往里面增加</p>\n<p>{‘role’:’user’, ‘content’:’…’}</p>\n<p>{‘role’:’assistent’, ‘content’:’…’}</p>\n<p>![image.png]([Vol][6]+语言模型+f5d8e690-b777-4214-8b28-491bf3ce268c/image 2.png)</p>\n<hr>\n<p>计算token 数量</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># callback ： token 计算</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain.callbacks <span class=\"keyword\">import</span> get_openai_callback</span><br><span class=\"line\"><span class=\"keyword\">with</span> get_openai_callback() <span class=\"keyword\">as</span> cb:</span><br><span class=\"line\">    result = azure_chat.invoke(<span class=\"string\">&quot;Tell me a joke&quot;</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(cb)</span><br><span class=\"line\"><span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<p>结果</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\">Tokens Used: <span class=\"number\">34</span></span><br><span class=\"line\">\tPrompt Tokens: <span class=\"number\">11</span></span><br><span class=\"line\">\tCompletion Tokens: <span class=\"number\">23</span></span><br><span class=\"line\">Successful Requests: <span class=\"number\">1</span></span><br><span class=\"line\">Total Cost (USD): $<span class=\"number\">6.25e-05</span>  ($<span class=\"number\">0.0000625</span>)</span><br></pre></td></tr></table></figure>\n\n<p>完整代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> uuid</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain.memory <span class=\"keyword\">import</span> ConversationBufferMemory</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models.azure_openai <span class=\"keyword\">import</span> AzureChatOpenAI</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.chat_models.baidu_qianfan_endpoint <span class=\"keyword\">import</span> QianfanChatEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.embeddings <span class=\"keyword\">import</span> QianfanEmbeddingsEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.llms.baidu_qianfan_endpoint <span class=\"keyword\">import</span> QianfanLLMEndpoint</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_community.vectorstores.chroma <span class=\"keyword\">import</span> Chroma</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.messages <span class=\"keyword\">import</span> HumanMessage, SystemMessage</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.output_parsers <span class=\"keyword\">import</span> StrOutputParser, CommaSeparatedListOutputParser</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompt_values <span class=\"keyword\">import</span> ChatPromptValue</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.prompts <span class=\"keyword\">import</span> ChatPromptTemplate, PromptTemplate, MessagesPlaceholder</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.runnables <span class=\"keyword\">import</span> RunnablePassthrough, RunnableParallel, RunnableLambda, ConfigurableField</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> operator <span class=\"keyword\">import</span> itemgetter</span><br><span class=\"line\"><span class=\"keyword\">from</span> langchain_core.pydantic_v1 <span class=\"keyword\">import</span> BaseModel, Field</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\"></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_ACCESS_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_ACCESS_KEY&#x27;</span>)</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;QIANFAN_SECRET_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_QIANFAN_SECRET_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    unique_id = uuid.uuid4().<span class=\"built_in\">hex</span>[<span class=\"number\">0</span>:<span class=\"number\">8</span>]</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;LANGCHAIN_PROJECT&quot;</span>] = <span class=\"string\">f&quot; [返回docs]轨迹 - <span class=\"subst\">&#123;unique_id&#125;</span>&quot;</span></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;LANGCHAIN_TRACING_V2&quot;</span>] = <span class=\"string\">&#x27;true&#x27;</span></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;LANGCHAIN_API_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_LANGCHAIN_API_KEY&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># chat</span></span><br><span class=\"line\">    chat_model = QianfanChatEndpoint(</span><br><span class=\"line\">        model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># llm</span></span><br><span class=\"line\">    llm = QianfanLLMEndpoint(model=<span class=\"string\">&quot;ERNIE-Bot-4&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    text = <span class=\"string\">&quot;讲一个狗熊的笑话&quot;</span></span><br><span class=\"line\">    messages = [HumanMessage(content=text)]</span><br><span class=\"line\"></span><br><span class=\"line\">    llm_res = llm.invoke(text)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># chat_res = chat_model.invoke(messages)</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 提示词  PromptTemplate</span></span><br><span class=\"line\">    prompt = PromptTemplate.from_template(<span class=\"string\">&quot;生产 &#123;product&#125; 的公司在哪里?&quot;</span>)</span><br><span class=\"line\">    prompt.<span class=\"built_in\">format</span>(product=<span class=\"string\">&quot;袜子&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#  ChatPromptTemplate</span></span><br><span class=\"line\">    template = <span class=\"string\">&quot;你是个专业的翻译，你把 &#123;input_language&#125; 翻译到 &#123;output_language&#125;.&quot;</span></span><br><span class=\"line\">    human_template = <span class=\"string\">&quot;&#123;text&#125;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    chat_prompt = ChatPromptTemplate.from_messages([</span><br><span class=\"line\">        (<span class=\"string\">&quot;system&quot;</span>, template),</span><br><span class=\"line\">        (<span class=\"string\">&quot;human&quot;</span>, human_template),</span><br><span class=\"line\">    ])</span><br><span class=\"line\"></span><br><span class=\"line\">    chat_prompt.format_messages(input_language=<span class=\"string\">&quot;English&quot;</span>, output_language=<span class=\"string\">&quot;Chinese&quot;</span>, text=<span class=\"string\">&quot;I love programming.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 消息</span></span><br><span class=\"line\">    messages = [</span><br><span class=\"line\">        SystemMessage(content=<span class=\"string\">&quot;你是一个AI助手&quot;</span>),</span><br><span class=\"line\">        HumanMessage(content=<span class=\"string\">&quot;今天有什么新闻？&quot;</span>),</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># invoke</span></span><br><span class=\"line\">    res = chat_model.invoke(messages)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># stream</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> chunk <span class=\"keyword\">in</span> chat_model.stream(messages):</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(chunk.content, end=<span class=\"string\">&quot;&quot;</span>, flush=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># function call</span></span><br><span class=\"line\">    <span class=\"keyword\">class</span> <span class=\"title class_\">Multiply</span>(<span class=\"title class_ inherited__\">BaseModel</span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot; 将2个数相加&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        a: <span class=\"built_in\">int</span> = Field(..., description=<span class=\"string\">&quot;第一个整数&quot;</span>)</span><br><span class=\"line\">        b: <span class=\"built_in\">int</span> = Field(..., description=<span class=\"string\">&quot;第二个整数&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># langchain的百度接口不支持</span></span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;AZURE_OPENAI_API_KEY&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_AZURE_OPENAI_API_KEY&#x27;</span>)</span><br><span class=\"line\">    os.environ[<span class=\"string\">&quot;AZURE_OPENAI_ENDPOINT&quot;</span>] = os.getenv(<span class=\"string\">&#x27;MY_AZURE_OPENAI_ENDPOINT&#x27;</span>)</span><br><span class=\"line\">    DEPLOYMENT_NAME_GPT3P5 = os.getenv(<span class=\"string\">&#x27;MY_DEPLOYMENT_NAME_GPT3P5&#x27;</span>)</span><br><span class=\"line\">    azure_chat = AzureChatOpenAI(</span><br><span class=\"line\">        openai_api_version=<span class=\"string\">&quot;2024-02-15-preview&quot;</span>,</span><br><span class=\"line\">        azure_deployment=DEPLOYMENT_NAME_GPT3P5,</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    function = &#123;</span><br><span class=\"line\">        <span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;add_two_int&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;将输入的2个整数相加&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;parameters&quot;</span>: &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;object&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;properties&quot;</span>: &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;int_a&quot;</span>: &#123;</span><br><span class=\"line\">                    <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;一个整数&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;string&quot;</span></span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                <span class=\"string\">&quot;int_b&quot;</span>: &#123;</span><br><span class=\"line\">                    <span class=\"string\">&quot;description&quot;</span>: <span class=\"string\">&quot;一个整数&quot;</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;type&quot;</span>: <span class=\"string\">&quot;string&quot;</span></span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            <span class=\"string\">&quot;required&quot;</span>: [<span class=\"string\">&quot;int_a&quot;</span>, <span class=\"string\">&quot;int_b&quot;</span>]</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 绑定运行时参数</span></span><br><span class=\"line\">    chat_with_tools = azure_chat.bind(</span><br><span class=\"line\">      function_call=&#123;<span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;add_two_int&quot;</span>&#125;, functions=[function]</span><br><span class=\"line\">    )</span><br><span class=\"line\">    <span class=\"comment\"># res = chat_with_tools.invoke(&quot;3加18等于多少&quot;)</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    res = azure_chat.with_config(configurable=&#123;<span class=\"string\">&quot;llm_temperature&quot;</span>: <span class=\"number\">0.9</span>&#125;).invoke(<span class=\"string\">&quot;给出一个随机数&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    llm = QianfanChatEndpoint(temperature=<span class=\"number\">0.7</span>).configurable_alternatives(</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 设置可以配置的项目</span></span><br><span class=\"line\">        ConfigurableField(<span class=\"built_in\">id</span>=<span class=\"string\">&quot;llm&quot;</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 设置缺省值，默认用百度</span></span><br><span class=\"line\">        default_key=<span class=\"string\">&quot;baidu4&quot;</span>,</span><br><span class=\"line\">        <span class=\"comment\"># 可以选择 Azure OpenAI</span></span><br><span class=\"line\">        azure_openai=azure_chat,</span><br><span class=\"line\">    )</span><br><span class=\"line\">    chain = prompt | llm</span><br><span class=\"line\">    <span class=\"comment\"># res_ = chain.with_config(configurable=&#123;&quot;llm&quot;: &quot;baidu4&quot;&#125;).invoke(&#123;&quot;product&quot;: &quot;铅笔&quot;&#125;)</span></span><br><span class=\"line\">    <span class=\"comment\"># res_ = chain.with_config(configurable=&#123;&quot;llm&quot;: &quot;azure_openai&quot;&#125;).invoke(&#123;&quot;product&quot;: &quot;铅笔&quot;&#125;)</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#</span></span><br><span class=\"line\">    prompt = ChatPromptTemplate.from_messages(</span><br><span class=\"line\">        [</span><br><span class=\"line\">            (<span class=\"string\">&quot;system&quot;</span>, <span class=\"string\">&quot;你是AI助手&quot;</span>),</span><br><span class=\"line\">            MessagesPlaceholder(variable_name=<span class=\"string\">&quot;history&quot;</span>),</span><br><span class=\"line\">            (<span class=\"string\">&quot;human&quot;</span>, <span class=\"string\">&quot;&#123;input&#125;&quot;</span>),</span><br><span class=\"line\">        ]</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    memory = ConversationBufferMemory(return_messages=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    res = memory.load_memory_variables(&#123;&#125;)</span><br><span class=\"line\">    chain = (</span><br><span class=\"line\">            RunnablePassthrough.assign(</span><br><span class=\"line\">                history=RunnableLambda(memory.load_memory_variables) | itemgetter(<span class=\"string\">&quot;history&quot;</span>)</span><br><span class=\"line\">            )</span><br><span class=\"line\">            | prompt</span><br><span class=\"line\">            | chat_model</span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\">    inputs = &#123;<span class=\"string\">&quot;input&quot;</span>: <span class=\"string\">&quot;你好！我是茉卷&quot;</span>&#125;</span><br><span class=\"line\">    response = chain.invoke(inputs)</span><br><span class=\"line\"></span><br><span class=\"line\">    memory.save_context(inputs, &#123;<span class=\"string\">&quot;output&quot;</span>: response.content&#125;)</span><br><span class=\"line\">    res = memory.load_memory_variables(&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 测试记忆</span></span><br><span class=\"line\">    inputs = &#123;<span class=\"string\">&quot;input&quot;</span>: <span class=\"string\">&quot;我叫什么名字&quot;</span>&#125;</span><br><span class=\"line\">    response = chain.invoke(inputs)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\">    memory.save_context(inputs, &#123;<span class=\"string\">&quot;output&quot;</span>: response.content&#125;)</span><br><span class=\"line\">    res = memory.load_memory_variables(&#123;&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># callback ： token 计算</span></span><br><span class=\"line\">    <span class=\"keyword\">from</span> langchain.callbacks <span class=\"keyword\">import</span> get_openai_callback</span><br><span class=\"line\">    <span class=\"keyword\">with</span> get_openai_callback() <span class=\"keyword\">as</span> cb:</span><br><span class=\"line\">        result = azure_chat.invoke(<span class=\"string\">&quot;Tell me a joke&quot;</span>)</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(cb)</span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>","tags":["langchain"]},{"title":"iterm2常用快捷键","url":"/forward/10883bf2.html","content":"<h1 id=\"Iterm2-常用快捷键\"><a href=\"#Iterm2-常用快捷键\" class=\"headerlink\" title=\"Iterm2 常用快捷键\"></a>Iterm2 常用快捷键</h1><p><img src=\"/images/iterm2%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE.assets/image-20240429133734551.png\" alt=\"image-20240429133734551\"></p>\n<h2 id=\"标签控制\"><a href=\"#标签控制\" class=\"headerlink\" title=\"标签控制\"></a>标签控制</h2><figure class=\"highlight text\"><table><tr><td class=\"code\"><pre><span class=\"line\">新建标签：command + t</span><br><span class=\"line\"></span><br><span class=\"line\">关闭标签：command + w</span><br><span class=\"line\"></span><br><span class=\"line\">切换标签：command + 数字 command + 左右方向键</span><br><span class=\"line\"></span><br><span class=\"line\">切换全屏：command + enter</span><br><span class=\"line\"></span><br><span class=\"line\">查找：command + f</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"分屏控制\"><a href=\"#分屏控制\" class=\"headerlink\" title=\"分屏控制\"></a>分屏控制</h2><figure class=\"highlight text\"><table><tr><td class=\"code\"><pre><span class=\"line\">垂直分屏：command + d</span><br><span class=\"line\"></span><br><span class=\"line\">水平分屏：command + shift + d</span><br><span class=\"line\"></span><br><span class=\"line\">切换屏幕：command + option + 方向键 command + [ 或 command + ]</span><br><span class=\"line\"></span><br><span class=\"line\">查看历史命令：command + ;</span><br><span class=\"line\"></span><br><span class=\"line\">查看剪贴板历史：command + shift + h</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"光标操作（常用）\"><a href=\"#光标操作（常用）\" class=\"headerlink\" title=\"光标操作（常用）\"></a>光标操作（常用）</h2><figure class=\"highlight text\"><table><tr><td class=\"code\"><pre><span class=\"line\">清除当前行（实际上是光标前全部内容）：ctrl + u</span><br><span class=\"line\"></span><br><span class=\"line\">删除前一个字符|删除后一个字符：ctrl + h | ctrl + h</span><br><span class=\"line\"></span><br><span class=\"line\">按单词往前删除（推荐记忆）：ctrl + w</span><br><span class=\"line\"></span><br><span class=\"line\">删除光标后所有内容：ctrl + k</span><br><span class=\"line\"></span><br><span class=\"line\">到行首（尾）：ctrl + a/e</span><br><span class=\"line\"></span><br><span class=\"line\">光标前进后退：-&gt; ctrl + f | &lt;- ctrl + b</span><br><span class=\"line\"></span><br><span class=\"line\">上一条命令|下一条命令：ctrl + p/n</span><br><span class=\"line\"></span><br><span class=\"line\">可以搜索的历史命令：ctrl + r</span><br><span class=\"line\"></span><br><span class=\"line\">清屏：command + r | ctrl + l | 我自己一般手动clear</span><br></pre></td></tr></table></figure>\n\n\n\n<p>上面就是我整理的一些快捷键。</p>\n<p>下面就是大整合：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"code\"><pre><span class=\"line\">新建标签：<span class=\"built_in\">command</span> + t</span><br><span class=\"line\"></span><br><span class=\"line\">关闭标签：<span class=\"built_in\">command</span> + w</span><br><span class=\"line\"></span><br><span class=\"line\">切换标签：<span class=\"built_in\">command</span> + 数字 <span class=\"built_in\">command</span> + 左右方向键</span><br><span class=\"line\"></span><br><span class=\"line\">切换全屏：<span class=\"built_in\">command</span> + enter</span><br><span class=\"line\"></span><br><span class=\"line\">查找：<span class=\"built_in\">command</span> + f</span><br><span class=\"line\"></span><br><span class=\"line\">垂直分屏：<span class=\"built_in\">command</span> + d</span><br><span class=\"line\"></span><br><span class=\"line\">水平分屏：<span class=\"built_in\">command</span> + <span class=\"built_in\">shift</span> + d</span><br><span class=\"line\"></span><br><span class=\"line\">切换屏幕：<span class=\"built_in\">command</span> + option + 方向键 <span class=\"built_in\">command</span> + [ 或 <span class=\"built_in\">command</span> + ]</span><br><span class=\"line\"></span><br><span class=\"line\">查看历史命令：<span class=\"built_in\">command</span> + ;</span><br><span class=\"line\"></span><br><span class=\"line\">查看剪贴板历史：<span class=\"built_in\">command</span> + <span class=\"built_in\">shift</span> + h</span><br><span class=\"line\"></span><br><span class=\"line\">清除当前行：ctrl + u</span><br><span class=\"line\"></span><br><span class=\"line\">到行首：ctrl + a</span><br><span class=\"line\"></span><br><span class=\"line\">到行尾：ctrl + e</span><br><span class=\"line\"></span><br><span class=\"line\">前进后退：ctrl + f/b (相当于左右方向键)</span><br><span class=\"line\"></span><br><span class=\"line\">上一条命令：ctrl + p</span><br><span class=\"line\"></span><br><span class=\"line\">搜索命令历史：ctrl + r</span><br><span class=\"line\"></span><br><span class=\"line\">删除当前光标的字符：ctrl + d</span><br><span class=\"line\"></span><br><span class=\"line\">删除光标之前的字符：ctrl + h</span><br><span class=\"line\"></span><br><span class=\"line\">删除光标之前的单词：ctrl + w</span><br><span class=\"line\"></span><br><span class=\"line\">删除到文本末尾：ctrl + k</span><br><span class=\"line\"></span><br><span class=\"line\">交换光标处文本：ctrl + t</span><br><span class=\"line\"></span><br><span class=\"line\">清屏1：<span class=\"built_in\">command</span> + r</span><br><span class=\"line\"></span><br><span class=\"line\">清屏2：ctrl + l</span><br><span class=\"line\"></span><br><span class=\"line\">自带有哪些很实用的功能/快捷键</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + 数字在各 tab 标签直接来回切换</span><br><span class=\"line\"></span><br><span class=\"line\">选择即复制 + 鼠标中键粘贴，这个很实用</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + f 所查找的内容会被自动复制</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + d 横着分屏 / ⌘ + <span class=\"built_in\">shift</span> + d 竖着分屏</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + r = clear，而且只是换到新一屏，不会想 clear 一样创建一个空屏</span><br><span class=\"line\"></span><br><span class=\"line\">ctrl + u 清空当前行，无论光标在什么位置</span><br><span class=\"line\"></span><br><span class=\"line\">输入开头命令后 按 ⌘ + ; 会自动列出输入过的命令</span><br><span class=\"line\"></span><br><span class=\"line\">⌘ + <span class=\"built_in\">shift</span> + h 会列出剪切板历史</span><br><span class=\"line\"></span><br><span class=\"line\">可以在 Preferences &gt; keys 设置全局快捷键调出 iterm，这个也可以用过 Alfred 实现</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n","categories":["折腾"],"tags":["MacOS终端"]},{"title":"Hello World","url":"/forward/4a17b156.html","content":"<h2 id=\"失語時代下的喃喃自語\"><a href=\"#失語時代下的喃喃自語\" class=\"headerlink\" title=\"失語時代下的喃喃自語\"></a>失語時代下的喃喃自語</h2><p>纪念2017年愚人节，reddit网站发起一项为期三天的社会实验（2017年4月1日-4月3日），号召所有注册用户在一块100万像素的画布上作画（1000*1000），用户有16种像素颜色选择，生成一次后需要等待20分钟到5分钟之后才可以进行下一次编辑。凭借大家的协作创造出得一幅伟大的作品并载入互联网史册。</p>\n<p>在混乱中建立秩序，文明也在一次次破坏重建中焕发了新的面貌，当资源有限的情况下，一个群体想要生存势必要蚕食别的群体，生存还是毁灭的问题在短短的72小时内在一块小小的帆布画版上不断上演。虽然这个活动在几年前就已经结束了，但现实中比Reddit这场社会实验残酷百倍的故事却从未停歇。<br><del>国家之间的冲突，族群之间的恶意。在更大纬度的战场，渗透于各个领域的对垒，甚至已经关乎到十几亿几十亿人们的幸福生活。</del></p>\n<p>活动地址参见：<a href=\"https://www.reddit.com/r/place/\">reddit/r/place</a><br>活动详情参见：<a href=\"https://en.wikipedia.org/wiki/Place_(Reddit)\">维基百科</a><br>画板元素详解：<a href=\"https://draemm.li/various/place-atlas/\">The /r/place Atlas</a><br>变化过程参见：<a href=\"https://www.bilibili.com/video/BV1WW41197qY\">bilibil</a></p>\n<p>活动结束时最终快照[高清]：<br><img src=\"/images/img--1.png\" alt=\"reddit-place-2017\"></p>\n","tags":["place"]},{"title":"计算机硬件基础-cache-校验码","url":"/forward/20906a71.html","content":"<h1 id=\"计算机基础硬件-cache-校验码\"><a href=\"#计算机基础硬件-cache-校验码\" class=\"headerlink\" title=\"计算机基础硬件-cache + 校验码\"></a>计算机基础硬件-cache + 校验码</h1><h2 id=\"cache\"><a href=\"#cache\" class=\"headerlink\" title=\"cache\"></a>cache</h2><p>功能:提高cpu数据输入输出的速率，突破冯诺依曼瓶颈</p>\n<p>速度：在计算机存储体系中，cache是访问速度较快的层次</p>\n<p>原理: 在使用cache改善系统性能的一居室程序的局部性原理。</p>\n<p>组成：cache由控制部分和存储部分组成</p>\n<p><img src=\"/images/pasted-2.png\" alt=\"upload successful\"></p>\n<p><img src=\"/images/pasted-3.png\" alt=\"upload successful\"></p>\n<h2 id=\"输入输出\"><a href=\"#输入输出\" class=\"headerlink\" title=\"输入输出\"></a>输入输出</h2><p><img src=\"/images/pasted-4.png\" alt=\"upload successful\"></p>\n<h2 id=\"校验码-奇偶校验码-，CRC-海明校验码\"><a href=\"#校验码-奇偶校验码-，CRC-海明校验码\" class=\"headerlink\" title=\"校验码-奇偶校验码 ，CRC,海明校验码\"></a>校验码-奇偶校验码 ，CRC,海明校验码</h2><p><img src=\"/images/pasted-5.png\" alt=\"upload successful\"></p>\n<p>奇偶校验码 - 只能检错，可检验1（奇数）位错</p>\n<p>CRC - 只能检错，可检多位</p>\n<p>海明码：可以检错纠错，和1位多位</p>\n<p><img src=\"/images/pasted-6.png\" alt=\"upload successful\"><br>奇偶校验很简单比如：</p>\n<p><span style=\"color:red\">1</span>|00  0 这样子这个就有一个奇数校验</p>\n<p><span style=\"color:red\">0</span>|01      1</p>\n<p><span style=\"color:red\">0</span>|10      2</p>\n<p><span style=\"color:red\">1</span>|11      3</p>\n<p>假如出现一个 <span style=\"color:red\">0</span>|11 这种就在奇数校验下非法了</p>\n<p>CRC 就是k个数据位+r个校验位没啥好说的</p>\n<p>海明码假设由48个数据位</p>\n<p>那么由公式 2^k &gt;= 48 + k     得出k = 6 6个校验位</p>\n<p><img src=\"/images/pasted-7.png\" alt=\"upload successful\"></p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"计算机组成原理指令存储-软考版","url":"/forward/8f372709.html","content":"<h1 id=\"指令-存储软考版本\"><a href=\"#指令-存储软考版本\" class=\"headerlink\" title=\"指令+存储软考版本\"></a>指令+存储软考版本</h1><h2 id=\"指令\"><a href=\"#指令\" class=\"headerlink\" title=\"指令\"></a>指令</h2><p><strong>一条指令就是机器语言的一个语句，是一组有意义的二进制代码</strong></p>\n<p>一条指令其实包含如下内容：”操作码字段” ，”地址码字段”</p>\n<ul>\n<li>操作码字段 - 指出计算机要执行什么性质的操作。</li>\n<li>地址码字段 - 包含各操作数的地址与结果存放地址。</li>\n</ul>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240417225856055.png\" alt=\"image-20240417225856055\"></p>\n<p>如果没有A1 A2 A3只有OP就是0地址指令符</p>\n<h2 id=\"寻址方式\"><a href=\"#寻址方式\" class=\"headerlink\" title=\"寻址方式\"></a>寻址方式</h2><ul>\n<li><p>立即寻址：地址码部分存放的就是操作数</p>\n</li>\n<li><p>直接寻址：地址码存放的是操作数的地址</p>\n</li>\n<li><p>间接寻址：地址码存放的是记录操作数地址的地址。</p>\n</li>\n<li><p>寄存器寻址：地址码部分告诉我们数据存在哪一个寄存器</p>\n</li>\n<li><p>寄存器间接寻址：数据存在哪一个寄存器的地址</p>\n</li>\n<li><p>———————–上面软考常考</p>\n</li>\n<li><p>下面这三个基本上就是加偏移量进行寻址</p>\n</li>\n<li><p>相对寻址-一般个电脑就这个</p>\n</li>\n<li><p>基址寻址</p>\n</li>\n<li><p>变址寻址</p>\n</li>\n</ul>\n<h2 id=\"计算机体系结构分类\"><a href=\"#计算机体系结构分类\" class=\"headerlink\" title=\"计算机体系结构分类\"></a>计算机体系结构分类</h2><table>\n<thead>\n<tr>\n<th>体系结构类型</th>\n<th>结构</th>\n<th>关键特性</th>\n<th>代表</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>单指令流，单数据流，<strong>SISD</strong></td>\n<td>控制部分：1<br />处理部件：1</td>\n<td></td>\n<td>单处理器系统</td>\n</tr>\n<tr>\n<td>单指令流，多数据流 <strong>SIMD</strong></td>\n<td>控制部分：1<br />处理部件：多个</td>\n<td>以同步的形式执行同一条指令</td>\n<td>阵列处理机，超级向量处理机</td>\n</tr>\n<tr>\n<td>多指令流，单数据流 <strong>MISD</strong></td>\n<td>控制部分：多个<br />处理部分：1</td>\n<td>不可能且不实际</td>\n<td>目前没有，有点像流水线之类的<br /></td>\n</tr>\n<tr>\n<td>多指令流，多数据流<br /><strong>MSMD</strong></td>\n<td>控制部分：多个<br />处理部分：多个</td>\n<td>能够实现作业任务，指令等各级全面执行</td>\n<td>多处理机系统，多计算机</td>\n</tr>\n</tbody></table>\n<p>阵列处理机：就是多台处理机组成，每台处理机处理相同任务，并行计算。</p>\n<p>多处理机系统：多台处理机设备组成的系统，每台处理机有属于自己的控制部件，可以执行独立的程序，共享一个主存储和所有外部设备。</p>\n<p><img src=\".//images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240418203026838.png\" alt=\"image-20240418203026838\"></p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011317032.png\" alt=\"image-20240424011317032\"></p>\n<h2 id=\"CISC-与-RISC\"><a href=\"#CISC-与-RISC\" class=\"headerlink\" title=\"CISC 与 RISC\"></a>CISC 与 RISC</h2><table>\n<thead>\n<tr>\n<th>–</th>\n<th>CISC（复杂）</th>\n<th>RISC(精简)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>指令</td>\n<td>数量多，使用频率差别大，可变长格式</td>\n<td>数量少<br />使用频率接近<br />定长格式<br />大部分为单周期指令<br />操作寄存器<br />只有Load/Store操作内存</td>\n</tr>\n<tr>\n<td>寻址方式</td>\n<td>支持多种</td>\n<td>支持方式少</td>\n</tr>\n<tr>\n<td>实现方式</td>\n<td>微程序控制技术</td>\n<td>增加了通用寄存器<br />硬布线逻辑控制为主<br />采用流水线<br /></td>\n</tr>\n<tr>\n<td>其他</td>\n<td></td>\n<td>优化编译，有效支持高级语言</td>\n</tr>\n</tbody></table>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011820711.png\" alt=\"image-20240424011820711\"></p>\n<h2 id=\"流水线\"><a href=\"#流水线\" class=\"headerlink\" title=\"流水线\"></a>流水线</h2><p>流水线：多条指令重叠进行操作的一种准并行处理实现技术。</p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011930035.png\" alt=\"image-20240424011930035\"></p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424011938087.png\" alt=\"image-20240424011938087\"></p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240424012012622.png\" alt=\"image-20240424012012622\"></p>\n<p>上面这个图是不是一下子看不懂？</p>\n<p>根据列题来说，流水线周期其实就是，三部分执行时间中最长的一部分，在如题中也就是2ns。</p>\n<p>那么流水线计算公式呢？</p>\n<p><em><em>单条指令所需时间+（n-1）</em> 流水线周期</em>*</p>\n<p>那么如题就是</p>\n<p>（2+2+1）+ 99 * 2 = 203</p>\n<h2 id=\"多级存储器结构\"><a href=\"#多级存储器结构\" class=\"headerlink\" title=\"多级存储器结构\"></a>多级存储器结构</h2><p>没啥好说的，看图即可，金字塔上面 贵和快和小。下面就是便宜慢和大</p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240418223757692.png\" alt=\"image-20240418223757692\"></p>\n<h2 id=\"存储器分类\"><a href=\"#存储器分类\" class=\"headerlink\" title=\"存储器分类\"></a>存储器分类</h2><p>一般就纠结一些</p>\n<p><img src=\"/images/%E6%8C%87%E4%BB%A4+%E5%AD%98%E5%82%A8-%E8%BD%AF%E8%80%83%E7%89%88.assets/image-20240418223824672.png\" alt=\"image-20240418223824672\"></p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"操作系统杂项考点习题-软考版","url":"/forward/cff67e5b.html","content":"<h1 id=\"操作系统\"><a href=\"#操作系统\" class=\"headerlink\" title=\"操作系统\"></a>操作系统</h1><h2 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h2><p>通过资源管理，提高计算机系统效率。</p>\n<p>改善人机界面，向用户提供友好的工作环境。</p>\n<h2 id=\"特性\"><a href=\"#特性\" class=\"headerlink\" title=\"特性\"></a>特性</h2><ul>\n<li>并发性</li>\n<li>共享性</li>\n<li>异步性</li>\n</ul>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240429123939629.png\" alt=\"image-20240429123939629\"></p>\n<h2 id=\"进程管理\"><a href=\"#进程管理\" class=\"headerlink\" title=\"进程管理\"></a>进程管理</h2><h3 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h3><p>进程：</p>\n<ol>\n<li>进程是程序的一次执行</li>\n<li>进程是一个程序及其数据在处理机上顺序执行时发生的活动</li>\n<li>进程是具有独立功能的程序在一个数据集合上运行的过程，是系统进行资源分配和调度的独立单位</li>\n<li>也可以这么记，进程是进程实体的一次运行，是系统进行资源分配和调度的一个独立单位</li>\n</ol>\n<p>程序：</p>\n<p>​    程序是一组有序指令的合集并存在某种介质上，本身不具有活动含义。</p>\n<p>线程:</p>\n<p>​    线程是进程中的一个实体，是被系统独立调用的基本单位。 线程的资源用的是进程里面的。</p>\n<p>简单的流程图（我的灵魂手绘图）：</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504112732155.png\" alt=\"image-20240504112732155\"></p>\n<h3 id=\"进程控制\"><a href=\"#进程控制\" class=\"headerlink\" title=\"进程控制\"></a>进程控制</h3><p>原语：控制程序的指令段，要么不执行，要么都执行，不可分割的。</p>\n<p>同步：直接制约，是有顺序的。</p>\n<p>互斥：异步制约，双方都可制约。</p>\n<p>灵界资源：一次只能供一个进程使用的资源</p>\n<p>临界区：使用临界资源的代码片段。</p>\n<h3 id=\"信号量机制\"><a href=\"#信号量机制\" class=\"headerlink\" title=\"信号量机制\"></a>信号量机制</h3><p>信号量：</p>\n<ul>\n<li>一个整数</li>\n<li>S&gt;=0 表示资源的可用数</li>\n<li>S&lt;0 S的绝对值就是等待队列或是阻塞队列的任务书</li>\n</ul>\n<p>PV操作：</p>\n<p>P操作就是占用一个资源，S -= 1</p>\n<p>S操作就是任务结束了，释放一个资源S+=1</p>\n<h4 id=\"PV互斥模型\"><a href=\"#PV互斥模型\" class=\"headerlink\" title=\"PV互斥模型\"></a>PV互斥模型</h4><p>在一个程序段里，pv操作应该同时出现。</p>\n<p>P(S):</p>\n<p>使用XXX</p>\n<p>V(S):</p>\n<p>后续代码</p>\n<p>互斥信号量s的初始值为1</p>\n<h4 id=\"PV同步模型\"><a href=\"#PV同步模型\" class=\"headerlink\" title=\"PV同步模型\"></a>PV同步模型</h4><p>如单缓冲区的生产消费问题，其实也就是同一个信号量在不同的模型中进行控制</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">smophore mutex = <span class=\"number\">1</span> empty = n full = <span class=\"number\">0</span></span><br><span class=\"line\">producer()&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        p(empty);</span><br><span class=\"line\">        p(mutex);</span><br><span class=\"line\">        produce;</span><br><span class=\"line\">       \tv(mutex);</span><br><span class=\"line\">        v(full);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">consumer()&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(<span class=\"number\">1</span>)&#123;</span><br><span class=\"line\">        p(full);</span><br><span class=\"line\">        p(mutex);</span><br><span class=\"line\">        consume ;</span><br><span class=\"line\">       \tv(mutex);</span><br><span class=\"line\">        v(empty);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<p>一张PPT的图：<br><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240430003954753.png\" alt=\"image-20240430003954753\"></p>\n<h2 id=\"存储管理\"><a href=\"#存储管理\" class=\"headerlink\" title=\"存储管理\"></a>存储管理</h2><h3 id=\"页式存储\"><a href=\"#页式存储\" class=\"headerlink\" title=\"页式存储\"></a>页式存储</h3><p>优点： 利用率高，碎片小，分配及管理简单</p>\n<p>缺点：增加了系统开销，可能产生抖动现象</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240430005646138.png\" alt=\"image-20240430005646138\"></p>\n<h3 id=\"段式存储\"><a href=\"#段式存储\" class=\"headerlink\" title=\"段式存储\"></a>段式存储</h3><p>段式存储主要是，程序段是完整的，不会和页式一样一个程序段被切成多块。</p>\n<p>优点：多道程序共享内存，各段程序修改互不影响。</p>\n<p>缺点：内存利用率低，内存碎片浪费大。</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240503232221396.png\" alt=\"image-20240503232221396\"></p>\n<h3 id=\"段页式存储\"><a href=\"#段页式存储\" class=\"headerlink\" title=\"段页式存储\"></a>段页式存储</h3><p>优点：空间浪费小，存储共享容易，存储保护容易，可以动态链接。</p>\n<p>缺点：由于管理软件的增加，复杂性和开销也增加，性能有所下降</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240503235441587.png\" alt=\"image-20240503235441587\"></p>\n<h2 id=\"虚拟存储器\"><a href=\"#虚拟存储器\" class=\"headerlink\" title=\"虚拟存储器\"></a>虚拟存储器</h2><p>其实实现使用也就是上文的存储管理。</p>\n<p><strong>好像考试也不怎么考这些</strong></p>\n<h3 id=\"页面置换算法\"><a href=\"#页面置换算法\" class=\"headerlink\" title=\"页面置换算法\"></a>页面置换算法</h3><ul>\n<li>先进先出FIFO</li>\n<li>最佳置换optimal</li>\n<li>最近最久未使用LRU</li>\n<li>最少使用LFU</li>\n</ul>\n<h3 id=\"文件组织结构\"><a href=\"#文件组织结构\" class=\"headerlink\" title=\"文件组织结构\"></a>文件组织结构</h3><p>逻辑结构：流式文件，记录式文件。</p>\n<p>物理结构：顺序结构，链接结构，索引结构。</p>\n<p><strong>好像考试也不怎么考这些</strong></p>\n<h3 id=\"虚设备与SPOOLING技术\"><a href=\"#虚设备与SPOOLING技术\" class=\"headerlink\" title=\"虚设备与SPOOLING技术\"></a>虚设备与SPOOLING技术</h3><p>当只有一个物理设备的情况下，有多个用户需要进行使用，所以就需要一个作业井（其实也就理解为一个任务队列）</p>\n<p><strong>随便贴图一张基本上不会考的样子。</strong></p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504112544771.png\" alt=\"image-20240504112544771\"></p>\n<h2 id=\"习题\"><a href=\"#习题\" class=\"headerlink\" title=\"习题\"></a>习题</h2><p>送分题：<br><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240429124040399.png\" alt=\"image-20240429124040399\"></p>\n<p><img src=\"%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240430004454017.png\" alt=\"image-20240430004454017\"></p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504000120592.png\" alt=\"image-20240504000120592\"></p>\n<p>这个题的思路就是，先判断在不在内存里，然后判断有没有访问过，最后判断有没有被修改过。</p>\n<p>实际上就是判断：去掉0访问，去掉0修改。</p>\n<p><img src=\"/images/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%BD%AF%E8%80%83%E4%B8%93%E9%A2%98.assets/image-20240504012327600.png\" alt=\"image-20240504012327600\"></p>\n<p>这个地方如果是字，那就是根据cpu的字长来32就是除32.</p>\n<p>如果是字节，那就是除8</p>\n","categories":["日记"],"tags":["信息系统管理工程师"]},{"title":"计算机组成原理部分（基础词汇扫盲+基本组成）-软考版","url":"/forward/624a8c21.html","content":"<h1 id=\"计算机基本组成\"><a href=\"#计算机基本组成\" class=\"headerlink\" title=\"计算机基本组成\"></a>计算机基本组成</h1><p><img src=\"/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.assets/image-20240415190659207.png\" alt=\"image-20240415190659207\"></p>\n<h2 id=\"计算机基组成\"><a href=\"#计算机基组成\" class=\"headerlink\" title=\"计算机基组成\"></a>计算机基组成</h2><ol>\n<li>输入设备</li>\n<li>运算器 控制器 ==》 CPU （其实在这个阶段还有CPU+主存储器被称为主机）还有说法就是CPU其实应该分为（运算器，寄存器组，控制器，内部总线）</li>\n<li>存储器</li>\n<li>输出设备</li>\n</ol>\n<p>其中比较值得记忆的就是<code>运算器</code>和<code>控制器</code></p>\n<h3 id=\"运算器\"><a href=\"#运算器\" class=\"headerlink\" title=\"运算器\"></a>运算器</h3><table>\n<thead>\n<tr>\n<th>模块</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>算数逻辑单元ALU</td>\n<td>进行算式计算和逻辑运算</td>\n</tr>\n<tr>\n<td>累加寄存器（有的设备没有累加寄存器直接用<code>数据缓冲寄存器替代</code>）</td>\n<td>存放数据运算的一个操作数或者结果如31+1=32中的32.也有可能是其中的31。因为它只能存一个。</td>\n</tr>\n<tr>\n<td>数据缓冲寄存器</td>\n<td>保存cpu的运算数和运算结果。<code>这个存的多一些</code></td>\n</tr>\n<tr>\n<td>状态条件寄存器</td>\n<td>在计算机中，它主要用来保存运算过程中的状态信息，比如运算结果、运算过程中的逻辑状态等。当运算出现异区状态时，它能够及时标识出来，帮助计算机更好地进行下一步的运算。</td>\n</tr>\n</tbody></table>\n<h3 id=\"控制器\"><a href=\"#控制器\" class=\"headerlink\" title=\"控制器\"></a>控制器</h3><table>\n<thead>\n<tr>\n<th>模块</th>\n<th>功能</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>程序计数器PC</td>\n<td>用于记录下一个需要运行的指令。</td>\n</tr>\n<tr>\n<td>指令寄存器IR</td>\n<td>存放当前运行的任务指令</td>\n</tr>\n<tr>\n<td>指令译码器</td>\n<td>将指令译码为计算机能执行内容</td>\n</tr>\n<tr>\n<td>时序部件</td>\n<td>实际上就是控制cpu 频率的</td>\n</tr>\n</tbody></table>\n<p><img src=\"/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.assets/image-20240415191622335.png\" alt=\"image-20240415191622335\"></p>\n<h2 id=\"并发并行\"><a href=\"#并发并行\" class=\"headerlink\" title=\"并发并行\"></a>并发并行</h2><h3 id=\"并发性\"><a href=\"#并发性\" class=\"headerlink\" title=\"并发性\"></a>并发性</h3><p>并发就是一时间段内运行的任务。</p>\n<h3 id=\"同时性\"><a href=\"#同时性\" class=\"headerlink\" title=\"同时性\"></a>同时性</h3><p>就是同一时刻。</p>\n<h2 id=\"词汇扫盲\"><a href=\"#词汇扫盲\" class=\"headerlink\" title=\"词汇扫盲\"></a>词汇扫盲</h2><p><img src=\"/images/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.assets/image-20240415191807006.png\" alt=\"image-20240415191807006\"></p>\n<h3 id=\"CPU的性能指标\"><a href=\"#CPU的性能指标\" class=\"headerlink\" title=\"CPU的性能指标\"></a>CPU的性能指标</h3><p>主频就是2.4GHZ这个，</p>\n<p>字长就是一次性能处理的2进制数据长度，如2^64    2^32这样子</p>\n<p>CPU缓存就是L1 L2高速缓存。</p>\n<h3 id=\"总线分类\"><a href=\"#总线分类\" class=\"headerlink\" title=\"总线分类\"></a>总线分类</h3><ul>\n<li>数据总线：顾名思义数据走向的总线。</li>\n<li>控制总线：控制指令总线</li>\n<li>地址总线：内存编址范围。（一般XP系统内存是4G，因为xp的操作系统内存编址是32位 2^32 = 2^2 * 2^30  然后 2^30是一个G，所以算下来就是4G）总之就是管理操作系统运行内存大小</li>\n</ul>\n<p>总线性能：带宽（这个就是是带宽），位宽（一般就是和CPU字长一样，32位就是32位），工作频率（时序频率）</p>\n<p>设备间连接方式：串行连接，和并行连接。 串行线只有一根但是可以很长，并行线可以有多根但是不能很长。前者速度慢但是距离长，后者速度快距离短。</p>\n","categories":["日记"],"tags":["信息系统管理工程师"]}]